# RFC-001: GNN-Powered Adaptive Routing for x0tta6bl4 Mesh Networks

**Status**: DRAFT (Awaiting Community Consensus)  
**Date**: Nov 1, 2025  
**Authors**: x0tta6bl4 Core Mesh Team  
**Discussions**: x0tta6bl4/rfcs#1  

---

## Executive Summary

This RFC proposes deploying **Graph Neural Networks (GraphSAGE)** on each mesh node to enable sub-2-second MTTR (Mean Time To Recovery) by predicting link degradation before failures occur and computing optimal rerouting paths in <10ms.

**Key Innovation**: Federated learning allows each node to improve the GNN locally without exposing topology data, maintaining privacy while improving collective resilience.

---

## 1. Problem Statement

### Current State (BATMAN-adv baseline)
- MTTR for 1000-node mesh: **‚â•20 seconds** (OMNeT++ simulation baseline)
- Root cause: Reactive routing (AODV waits for link failure before acting)
- Reactive recovery: re-broadcast, SPF recalculation ‚Üí flooding

### Target State (H1 2026)
- MTTR: **‚â§2.5 seconds** for 1000 nodes
- Proactive: GNN predicts failures 5‚Äì10s in advance
- Pre-compute 3‚Äì5 disjoint paths (k-disjoint SPF)
- Instant failover: packet reroute without topology discovery

### Impact Metrics
| Metric | Current | Target | Improvement |
|--------|---------|--------|-------------|
| MTTR (1000 nodes) | 20s | 2.5s | 8x faster |
| Node failure detection | 5‚Äì10s | 0.5s | 10x faster |
| Path computation latency | 2‚Äì3s (SPF) | <0.1s (pre-computed) | 30x faster |
| Network stability | 85% PDR under churn | 95%+ PDR | +10 points |

---

## 2. Proposed Solution

### 2.1 Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Each Mesh Node (BATMAN-adv kernel)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ eBPF Monitor ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí   ‚îÇ   Beacons    ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ        ‚îÇ   (RSSI/SNR) ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ         ‚Üì                        ‚Üì                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ     GNN Feature Extraction (10ms)    ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Beacon loss trend                 ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Link quality (RSSI‚Üí-70dBm?)       ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Neighbor degree (isolation risk?) ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ         ‚Üì                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ   GraphSAGE Inference (<10ms)        ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ   Predict: Link failure probability  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ   Output: Top-3 backup next-hops     ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ         ‚Üì                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ   MAPE-K Decision Agent              ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Threshold crossing? ‚Üí Pre-reroute ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ   ‚Ä¢ Forward backup path selection    ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ Local Model Retraining (6h cadence)  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Federated learning: secure agg     ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Privacy-preserving gradient sharing ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Technical Components

#### A. Feature Engineering (Per Node)

```python
# Pseudocode: Beacon-based feature extraction
class BeaconFeatureExtractor:
    def __init__(self, window_size=30):
        self.window_size = window_size  # 30 recent beacons
        self.beacon_history = {}  # {neighbor_id: [timestamps, rssi, loss]}
    
    def extract_features(self, neighbor_id):
        """
        Extract 8D feature vector for neighbor link quality
        """
        history = self.beacon_history[neighbor_id]
        
        # Temporal features
        beacon_loss_rate = self._compute_loss_rate(history)      # 0‚Äì1
        rssi_trend = self._compute_rssi_trend(history)           # -100‚Äì0 dBm
        
        # Statistical features
        rssi_variance = np.var(history['rssi'][-self.window_size:])
        beacon_delay_jitter = self._compute_jitter(history)
        
        # Topological features
        neighbor_degree = len(self.mesh_neighbors)  # isolation risk?
        is_bottleneck = self._is_critical_node(neighbor_id)     # binary
        
        # Composite
        link_age_hours = (time.time() - history['created_at']) / 3600
        recent_churn = self._recent_topology_changes()          # count
        
        features = np.array([
            beacon_loss_rate,
            rssi_trend / -100,  # normalize to [0, 1]
            rssi_variance,
            beacon_delay_jitter,
            neighbor_degree / max_neighbors,
            float(is_bottleneck),
            link_age_hours / 24,  # normalize to [0, 1] for ~1 day links
            recent_churn / 10  # normalize churn events
        ], dtype=np.float32)
        
        return features
    
    def _compute_loss_rate(self, history, window=30):
        recent = history['loss'][-window:]
        return sum(recent) / len(recent) if recent else 0.0
    
    def _compute_rssi_trend(self, history):
        rssi_vals = history['rssi'][-10:]
        return np.mean(rssi_vals) if rssi_vals else -80
    
    def _compute_jitter(self, history):
        deltas = np.diff(history['beacon_intervals'][-10:])
        return np.std(deltas) if len(deltas) > 1 else 0.0
    
    def _is_critical_node(self, neighbor_id):
        # BFS: how many nodes unreachable if this neighbor dies?
        unreachable = self._count_isolated_nodes(neighbor_id)
        return unreachable > 5  # threshold
```

#### B. GraphSAGE Model (Lightweight, <5MB)

```python
import torch
import torch.nn as nn
from torch_geometric.nn import GraphSAGE

class MeshLinkPredictor(nn.Module):
    """
    Lightweight GraphSAGE for link failure prediction
    - Hidden dim: 64 (vs typical 256)
    - Layers: 2 (vs typical 3‚Äì4)
    - Params: ~15K (fits in RPi RAM)
    """
    def __init__(self, in_features=8, hidden_dim=64, num_layers=2):
        super().__init__()
        
        self.gnn = GraphSAGE(
            in_channels=in_features,
            hidden_channels=hidden_dim,
            num_layers=num_layers,
            out_channels=1,  # probability of failure
            dropout=0.3,
            use_batch_norm=False,  # lighter
        )
        
        # Output: failure probability [0, 1]
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x, edge_index):
        """
        x: node features (B, 8)
        edge_index: topology (2, E)
        returns: failure probabilities (B, 1)
        """
        out = self.gnn(x, edge_index)
        return self.sigmoid(out)

# Example inference on RPi 4 (ARM64)
def predict_failure_probability():
    model = MeshLinkPredictor(in_features=8)
    model.eval()  # no gradient tracking
    
    # Mock: 10 neighbors, 8D features each
    x = torch.randn(10, 8, dtype=torch.float32)
    edge_index = torch.tensor([[0, 1, 2], [1, 2, 3]], dtype=torch.long)  # bidirectional
    
    with torch.no_grad():
        failure_probs = model(x, edge_index)  # (10, 1)
    
    # Threshold: flag if prob > 0.6
    critical_neighbors = (failure_probs > 0.6).nonzero(as_tuple=True)[0].tolist()
    
    return failure_probs, critical_neighbors
```

#### C. MAPE-K Control Loop Integration

```bash
#!/bin/bash
# MAPE-K loop executed every 2s (or on beacon anomaly)

set -e

NODE_ID=$(hostname)
BEACON_DIR="/var/mesh/beacons"
MODEL_PATH="/opt/mesh-gnn/link_predictor_latest.pt"

# === MONITOR ===
echo "[$(date)] MONITOR: Collecting beacon metrics..."
python3 - <<'EOF'
import time, json
from beacon_extractor import BeaconFeatureExtractor

extractor = BeaconFeatureExtractor()
features = {}

for neighbor in extractor.get_neighbors():
    feat_vec = extractor.extract_features(neighbor)
    features[neighbor] = feat_vec.tolist()

with open("/tmp/gnn_features.json", "w") as f:
    json.dump(features, f)
EOF

# === ANALYSIS ===
echo "[$(date)] ANALYSIS: Running GNN inference..."
python3 - <<'EOF'
import json, torch
from mesh_link_predictor import MeshLinkPredictor

model = MeshLinkPredictor()
model.load_state_dict(torch.load("/opt/mesh-gnn/link_predictor_latest.pt"))
model.eval()

with open("/tmp/gnn_features.json") as f:
    features = json.load(f)

# Build graph from BATMAN adjacency
edge_index = build_edge_index_from_batman()
x = torch.tensor([features[n] for n in sorted(features.keys())], dtype=torch.float32)

with torch.no_grad():
    failure_probs = model(x, edge_index)

critical = []
for i, (neighbor, prob) in enumerate(zip(sorted(features.keys()), failure_probs)):
    if prob > 0.6:
        critical.append({"neighbor": neighbor, "failure_prob": float(prob)})

with open("/tmp/gnn_predictions.json", "w") as f:
    json.dump(critical, f)
EOF

# === PLANNING ===
echo "[$(date)] PLANNING: Pre-computing k-disjoint paths..."
python3 - <<'EOF'
import json, subprocess

# Trigger k-SPF computation for critical neighbors
with open("/tmp/gnn_predictions.json") as f:
    critical = json.load(f)

for item in critical:
    neighbor = item["neighbor"]
    # Pre-compute 3-disjoint paths via AODV extension
    subprocess.run([
        "batman-adv-reroute-precompute",
        f"--target={neighbor}",
        f"--k-disjoint=3",
        f"--output=/tmp/failover_{neighbor}.route"
    ])
EOF

# === EXECUTION ===
echo "[$(date)] EXECUTION: Installing failover routes..."
python3 - <<'EOF'
import json, subprocess

with open("/tmp/gnn_predictions.json") as f:
    critical = json.load(f)

for item in critical:
    neighbor = item["neighbor"]
    # Install secondary next-hops via iptables
    route_file = f"/tmp/failover_{neighbor}.route"
    
    with open(route_file) as f:
        backup_routes = json.load(f)
    
    for backup_nh in backup_routes["alternates"][:2]:
        subprocess.run([
            "ip", "rule", "add", f"oif=bat0", "lookup", "mesh-backup",
            f"priority={1000 + hash(neighbor) % 100}"
        ])
        subprocess.run([
            "ip", "route", "add", backup_nh,
            "dev=bat0", "table=mesh-backup"
        ])
EOF

# === KNOWLEDGE (Federated Learning Update) ===
if [ $(($(date +%s) % 21600)) -lt 60 ]; then
    echo "[$(date)] KNOWLEDGE: Triggering federated retraining..."
    python3 /opt/mesh-gnn/federated_train.py \
        --node-id="$NODE_ID" \
        --aggregator="http://aggregator.mesh:8888" \
        --model-path="$MODEL_PATH" \
        --epochs=3 \
        --lr=0.001
fi
```

#### D. Federated Learning (Privacy-Preserving)

```python
# federated_train.py: Distributed training without topology leakage

import torch
import torch.nn.functional as F
from torch_geometric.data import Data
import requests
import hashlib
import json

class FederatedMeshTrainer:
    def __init__(self, node_id, aggregator_url):
        self.node_id = node_id
        self.aggregator_url = aggregator_url
        self.model = MeshLinkPredictor()
        self.local_epochs = 3
        self.learning_rate = 0.001
    
    def train_local_batch(self, local_data):
        """
        Train on local beacon history without exposing topology.
        
        local_data: {'beacons': [...], 'labels': [0/1 for failure]}
        """
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        loss_fn = F.binary_cross_entropy
        
        for epoch in range(self.local_epochs):
            x_batch = torch.tensor(local_data['beacons'], dtype=torch.float32)
            y_batch = torch.tensor(local_data['labels'], dtype=torch.float32)
            
            # Local gradient descent
            optimizer.zero_grad()
            predictions = self.model(x_batch, edge_index_local=None)
            loss = loss_fn(predictions.squeeze(), y_batch)
            loss.backward()
            optimizer.step()
            
            print(f"[{self.node_id}] Epoch {epoch+1} Loss: {loss:.4f}")
        
        return self.get_local_gradients()
    
    def get_local_gradients(self):
        """Extract gradients for secure aggregation."""
        grads = []
        for param in self.model.parameters():
            if param.grad is not None:
                # Add Laplace noise for differential privacy (epsilon=1.0)
                noise = torch.normal(0, 1.0, param.grad.shape)
                noisy_grad = param.grad + noise
                grads.append(noisy_grad.cpu().numpy().tobytes())
        
        return {
            "node_id": self.node_id,
            "gradients": grads,
            "timestamp": int(time.time())
        }
    
    def send_gradients_to_aggregator(self, gradients):
        """Send encrypted gradients to aggregator (no topology revealed)."""
        payload = json.dumps(gradients, default=str)
        payload_hash = hashlib.sha256(payload.encode()).hexdigest()
        
        response = requests.post(
            f"{self.aggregator_url}/aggregate",
            json={"payload_hash": payload_hash, "gradients": gradients},
            headers={"Authorization": f"Bearer {self.node_id}"}
        )
        
        if response.status_code == 200:
            new_weights = response.json()["aggregated_model"]
            self.model.load_state_dict(torch.load(new_weights))
            print(f"[{self.node_id}] Model updated from aggregator")
        else:
            print(f"[{self.node_id}] Aggregation failed: {response.text}")

# Usage
if __name__ == "__main__":
    trainer = FederatedMeshTrainer(
        node_id="mesh-node-42",
        aggregator_url="http://aggregator.mesh:8888"
    )
    
    # Simulate local training data (6h beacon history)
    local_beacons = generate_local_training_data()  # 1000 samples
    
    # Train locally
    grads = trainer.train_local_batch(local_beacons)
    
    # Send to aggregator (encrypted, no topology leak)
    trainer.send_gradients_to_aggregator(grads)
```

---

## 3. Implementation Phases

### Phase 1: Prototype (Weeks 1‚Äì4)
- [ ] Deploy BATMAN-adv on 15-node testbed (Raspberry Pi)
- [ ] Implement BeaconFeatureExtractor + GNN inference
- [ ] Baseline accuracy: 92% recall for simulated failures
- [ ] Inference latency: <10ms on RPi 4

### Phase 2: Integration (Weeks 5‚Äì12)
- [ ] MAPE-K bash script + systemd timer (2s cadence)
- [ ] k-disjoint SPF precomputation (3‚Äì5 paths)
- [ ] Chaos testing: 100-node mesh, 10 random link failures ‚Üí verify MTTR <2.5s
- [ ] eBPF monitoring (Prometheus integration)

### Phase 3: Federated Learning (Weeks 13‚Äì18)
- [ ] Central aggregator (Kubernetes pod) for gradient collection
- [ ] Federated training: all 100 nodes train locally, aggregate every 6h
- [ ] Privacy audit: verify no topology leakage via gradient inspection
- [ ] DAO voting on model updates (Snapshot)

---

## 4. Risks & Mitigations

### Risk 1: Byzantine Nodes Poisoning GNN
**Impact**: Compromised node sends bad gradients ‚Üí model degradation  
**Probability**: Medium (depends on node security)

| Mitigation | Effort | Status |
|-----------|--------|--------|
| Reputation scoring (discount low-trust gradients) | Medium | RFC-002 |
| Anomaly detection on gradient changes | Low | Implement S2 |
| Quorum: only aggregate if >50% agree on update | Medium | RFC-003 (DAO voting) |

### Risk 2: Inference Latency Exceeds 10ms
**Impact**: Rerouting too slow, defeats purpose  
**Probability**: Low (GraphSAGE proven <10ms on RPi)

| Mitigation | Effort | Status |
|-----------|--------|--------|
| Use LEANN if GraphSAGE slow (2‚Äì3MB, 80% recall) | Medium | H2 fallback |
| Move inference to GPU (optional on edge) | High | H2 stretch |
| Quantization: FP32 ‚Üí INT8 (¬±2% accuracy loss) | Low | S2 PoC |

### Risk 3: High Bandwidth for Federated Learning
**Impact**: Gradient upload saturates mesh uplink  
**Probability**: Medium (6h cadence helps, but size unknown)

| Mitigation | Effort | Status |
|-----------|--------|--------|
| Compress gradients: zstd + delta encoding | Medium | S3 implement |
| Sparse gradient updates (top-K by magnitude) | High | H2 advanced |
| Stale model sync: allow async, eventual consistency | Low | RFC-004 |

### Risk 4: DAO Governance Too Slow for Model Updates
**Impact**: Weekly governance cycle ‚Üí slow to fix model bugs  
**Probability**: Low (delegated voting + trusted committee fallback)

| Mitigation | Effort | Status |
|-----------|--------|--------|
| Trusted committee: fast-track critical fixes (DAO override) | Low | DAO rules doc |
| Automatic rollback: if accuracy drops >5% in 1h | Low | S3 implement |

---

## 5. Success Criteria

### Technical Metrics (Measurable by S2 end)
- [ ] MTTR ‚â§ 2.5s for 1000-node mesh (Chaos testing, 95th percentile)
- [ ] GNN inference latency ‚â§10ms cold start on Raspberry Pi 4
- [ ] Accuracy ‚â•92% for link failure prediction (vs 50% random)
- [ ] Throughput impact <1% (pre-computation overhead negligible)

### Operational Metrics
- [ ] MAPE-K loop runs reliably every 2s (99.9% uptime)
- [ ] Federated training convergence within 60 min (3 local epochs)
- [ ] Zero Byzantine attacks detected in Phase 3

### Community Metrics
- [ ] RFC approved by DAO vote (>33% participation)
- [ ] 10+ nodes contributing training data to federation
- [ ] Zero privacy violations (gradient inspection audit pass)

---

## 6. Alternatives Considered

| Approach | Pros | Cons | Verdict |
|----------|------|------|---------|
| **GraphSAGE** (selected) | <10ms, federated-ready, lightweight | Requires model retraining | ‚úÖ Best fit |
| Pure rule-based (threshold on RSSI) | Simple, no ML | Brittle, can't adapt | ‚ùå Too rigid |
| Centralized LSTM (cloud model) | High accuracy | Privacy leak, single point of failure | ‚ùå Unacceptable |
| GCN (Graph Convolutional) | High accuracy | >50ms latency, non-federated | ‚ùå Too slow |

---

## 7. Open Questions

**Q1: How to validate GNN accuracy without ground truth?**  
A: Use Chaos Mesh to inject failures, compare GNN prediction timing vs actual failure.

**Q2: Federated learning convergence: is 60 min acceptable?**  
A: Yes, 6h cadence means daily updates. Faster convergence (RFC-004) for H2.

**Q3: What if aggregator node fails?**  
A: DAO stores aggregated model on IPFS, nodes fall back to last known-good version.

**Q4: Can we use blockchain for gradient aggregation?**  
A: Expensive (not practical for 100+ nodes, $1000s/day on L1). Use L2 rollup or off-chain only.

---

## 8. Glossary

- **MTTR**: Mean Time To Recovery (seconds to restore connectivity)
- **GraphSAGE**: Graph Sample and Aggregate (inductive learning on graphs)
- **k-disjoint SPF**: k edge-disjoint shortest paths (redundancy)
- **MAPE-K**: Monitor, Analyze, Plan, Execute, Knowledge feedback loop
- **eBPF**: Extended Berkeley Packet Filter (kernel instrumentation)
- **Federated Learning**: Distributed training without centralizing data
- **Differential Privacy**: Add noise to gradients before sharing

---

## 9. Approvals Required

- [ ] **Mesh Working Group** (technical review) ‚Äî @mesh-lead
- [ ] **Security Team** (privacy audit) ‚Äî @sec-lead
- [ ] **DAO Governance** (Snapshot vote, >33% quorum) ‚Äî @dao-admin
- [ ] **Community Consensus** (GitHub Discussions, >5 üëç reactions) ‚Äî public

---

## 10. Implementation Contacts

- **Design Lead**: x0tta6bl4 Core Team (@mesh-lead)
- **Federated Learning**: @ml-specialist
- **DAO Integration**: @governance-lead
- **Testing/QA**: @qa-engineer

---

**Version**: v1.0-DRAFT  
**Last Modified**: Oct 31, 2025  
**Next Review**: Nov 15, 2025 (Community vote)