# Federated Learning Orchestrator Specification
## Scaling to 1000+ Node Mesh Network

**Status**: üìã **SPECIFICATION**  
**Date**: 2026-01-13  
**Target Scale**: 1000+ node federated learning  
**Architecture**: Asynchronous aggregation with Byzantine fault tolerance

---

## Executive Summary

This specification defines the Federated Learning Orchestrator for x0tta6bl4, enabling coordinated machine learning training across 1000+ mesh nodes. Three aggregation patterns optimize for different scenarios:

1. **Batch Async Aggregation**: Collect K < N updates, aggregate, distribute
2. **Streaming Aggregation**: Incremental updates with online convergence
3. **Hierarchical Aggregation**: Edge aggregators ‚Üí central coordinator (bandwidth efficient)

All patterns include Byzantine fault tolerance, straggler handling, and convergence validation.

### Key Features
‚úÖ Asynchronous model training (no waiting for stragglers)  
‚úÖ Hierarchical aggregation (scales to 10k+ nodes)  
‚úÖ Byzantine robust aggregation (detects 30%+ malicious nodes)  
‚úÖ Automatic convergence detection  
‚úÖ Integrated with mesh network  
‚úÖ SPIFFE identity validation for all updates  

---

## Architecture Overview

### System Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Central FL Orchestrator                      ‚îÇ
‚îÇ  (Convergence validation, learning rate scheduling) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇEdge Agg  ‚îÇ  ‚îÇEdge Agg     ‚îÇ  ‚îÇEdge Agg   ‚îÇ
‚îÇ(Zone 1)  ‚îÇ  ‚îÇ(Zone 2)     ‚îÇ  ‚îÇ(Zone 3)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                  ‚îÇ             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê
‚îÇN1 N2 ‚îÇN3‚îÇN4‚îÇ   ‚îÇN5 N6  ‚îÇN7‚îÇ   ‚îÇN8 N9 ‚îÇN10‚îÇ...‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò

Nodes: Training nodes updating local models
Edge Aggregators: Aggregate within zone (10-100 nodes)
Central: Aggregate zones, validate convergence
```

### Communication Flow

**Training Round**:
```
1. Central broadcasts current model to all zones
2. Each node trains locally (SGD, local epochs)
3. Nodes send gradient/weight updates to edge aggregator
4. Edge aggregators combine updates (weighted average)
5. Aggregators send combined updates to central
6. Central combines zone updates, validates convergence
7. If converged ‚Üí stop, else ‚Üí repeat
```

**Timeline for 1000 nodes with 3 zones (333 nodes per zone)**:
```
Time   Action                          Duration
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
T0     Broadcast model                 ~100ms
T100   Local training begins           ~5-60 seconds
T5100  Gradient collection starts      ~500ms
T5600  Edge aggregation                ~200ms
T5800  Central aggregation             ~100ms
T5900  Convergence check               ~100ms
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total per round:                       ~6 seconds
```

---

## Aggregation Patterns

### Pattern 1: Batch Async Aggregation

**Use Case**: Heterogeneous compute, variable stragglers

**Algorithm**:
```
Training Round R:
  1. Central sends current model M_R to all nodes
  2. All N nodes train locally:
     - Load M_R
     - Run SGD for E local epochs
     - Compute gradient ‚àáL_i
     - Return ‚àáL_i to central
  
  3. Central waits for K < N updates (threshold, not all)
     - Typically K = 80-95% of N
     - Timeout after T_max seconds (e.g., 60s)
  
  4. Central aggregates received updates:
     M_{R+1} = M_R - Œ± * mean(‚àáL_i for i in received)
     where Œ± = learning rate
  
  5. Stragglers (N-K) updates arrive late:
     - Discard old updates, use in next round
     - Option: Apply momentum from previous round
  
  6. Repeat from step 1
```

**Parameters**:
```yaml
K_threshold: 0.85          # Aggregate when 85% have updated
T_timeout: 60              # Max wait time (seconds)
local_epochs: 1-5          # Local training iterations
learning_rate: 0.01-0.1    # Central update rate
```

**Advantages**:
- ‚úÖ No waiting for slow nodes
- ‚úÖ Simple implementation
- ‚úÖ Fast convergence for homogeneous networks
- ‚úÖ Fault tolerant (slow node doesn't block)

**Disadvantages**:
- ‚ùå May skip updates from slow nodes
- ‚ùå Potential staleness issues
- ‚ùå No automatic learning rate adaptation

### Pattern 2: Streaming Aggregation

**Use Case**: Stable network, continuous training

**Algorithm**:
```
Streaming State:
  M = initial model
  v_sum = 0  # momentum term
  update_count = 0
  
For each arriving gradient update ‚àáL_i:
  1. Validate signature (SPIFFE identity)
  2. Check staleness: if (current_round - update_round) > MAX_STALENESS
     - Reject update
  
  3. Incremental update:
     v_sum = Œ≤ * v_sum + ‚àáL_i  # Momentum
     M = M - Œ± * v_sum / update_count
     update_count += 1
  
  4. Every K updates or T seconds:
     - Check convergence
     - Adjust learning rate
     - Log metrics
  
  5. Periodically broadcast M to nodes for validation
```

**Parameters**:
```yaml
momentum: 0.9              # Velocity damping
alpha: 0.01                # Learning rate
max_staleness: 5           # Max rounds delay
check_interval: 100        # Updates before convergence check
```

**Advantages**:
- ‚úÖ True online learning
- ‚úÖ Immediate use of updates
- ‚úÖ Natural load balancing
- ‚úÖ Automatic learning rate adaptation

**Disadvantages**:
- ‚ùå Complex state management
- ‚ùå Ordering dependencies
- ‚ùå Harder to debug convergence issues

### Pattern 3: Hierarchical Aggregation

**Use Case**: 1000+ nodes, bandwidth constrained

**Algorithm**:
```
Three-level hierarchy:

Level 1 - Edge Aggregators (100 per zone, 10 zones):
  For zone Z:
    1. Collect updates from nodes in Z (10-100 nodes)
    2. Aggregate locally:
       A_Z = mean(‚àáL_i for i in Z)
    3. Send A_Z to central (single gradient, not N)
    4. Receive M' from central
    5. Broadcast M' to local nodes

Level 2 - Central Coordinator:
  1. Wait for updates from K_edge < N_edges edge aggregators
  2. Combine zone updates:
     M_{R+1} = M_R - Œ± * mean(A_Z for Z in received)
  3. Validate convergence
  4. Broadcast M_{R+1} to all edge aggregators

Level 3 - Nodes:
  (Same as batch async, but aggregator is local)
```

**Bandwidth Savings**:
```
Single aggregation (no hierarchy):
  Bandwidth = N gradients √ó gradient_size
  = 1000 √ó 1MB = 1GB per round

Hierarchical (10 zones):
  Level 1: 100 gradients per zone √ó 10 zones = 1GB
  Level 2: 10 zone gradients = 10MB
  Total: ~1GB (same at L1, but L2 is minimal)
  
  BUT with compression:
  Each zone aggregates ‚Üí 1 gradient = 1MB
  10 zones ‚Üí 10MB to central
  Total: ~10MB (10x reduction)
```

**Parameters**:
```yaml
zones: 10                  # Number of edge aggregators
nodes_per_zone: 100        # ~1000 / 10
zone_aggregation_timeout: 30
central_aggregation_timeout: 60
```

**Advantages**:
- ‚úÖ Bandwidth efficient (10x reduction)
- ‚úÖ Scales to 10,000+ nodes
- ‚úÖ Natural fault boundaries
- ‚úÖ Reduced central coordinator load

**Disadvantages**:
- ‚ùå More complex coordination
- ‚ùå Potential information loss at edge
- ‚ùå Edge aggregator failure impacts zone

---

## Byzantine Fault Tolerance

### Threat Model
```
Adversary can control up to f nodes (typically f < N/3)
Adversary can:
  ‚úì Send incorrect gradient updates
  ‚úì Send gradient at wrong time
  ‚úì Collude with other malicious nodes
  ‚úó Cannot eavesdrop (TLS/SPIFFE encrypt)
  ‚úó Cannot forge SPIFFE identity (cryptographic)
  ‚úó Cannot modify honest nodes' updates (signed beacons)
```

### Byzantine Robust Aggregation

**Method 1: Coordinate-wise Median**
```python
# For each parameter in gradient:
param_updates = [update_i.param for update_i in received_updates]
# Take median instead of mean
aggregated_param = median(param_updates)

Guarantees:
  ‚úì Tolerates < 50% Byzantine nodes
  ‚úì Simple, no hyperparameters
  ‚úó May lose gradient information
  ‚úó Slower than mean
```

**Method 2: Geometric Median**
```python
# Find point closest to all updates
aggregated = geometric_median(received_updates)

Guarantees:
  ‚úì Tolerates < 50% Byzantine nodes
  ‚úì Preserves more gradient info than coordinate-wise median
  ‚úì Robust to collusion
  ‚úó Computationally expensive
```

**Method 3: Krum**
```python
# Select update with minimum sum of distances to others
for each update u:
    score = sum(distance(u, v) for v in updates)
selected = update with minimum score

Guarantees:
  ‚úì Tolerates up to f < (N-d-2)/2 Byzantine nodes
  ‚úì Preserves gradient well
  ‚úì Fast computation
  ‚úó Single update, may miss information
```

**Method 4: Multiround Filtering** (Recommended for x0tta6bl4)
```python
Round 1 - Outlier Detection:
  1. Compute distances between all pairs of updates
  2. Flag updates with high average distance to others
  3. Remove flagged updates (tolerance: up to 30%)

Round 2 - Aggregate Clean Updates:
  aggregated = mean(remaining_updates)

Guarantees:
  ‚úì Tolerates up to 30% Byzantine nodes
  ‚úì Fast computation
  ‚úì Preserves gradient information
  ‚úì Easy to implement and debug
```

### Implementation
```python
class ByzantineDetector:
    """Detects and filters Byzantine updates"""
    
    def __init__(self, tolerance_fraction: float = 0.30):
        """tolerance_fraction: fraction of nodes that can be Byzantine (0.30 = 30%)"""
        self.tolerance = tolerance_fraction
    
    def detect_malicious_updates(self, updates: List[Gradient]) -> List[int]:
        """Returns indices of suspected Byzantine updates"""
        if len(updates) < 10:  # Not enough samples
            return []
        
        # Compute pairwise distances
        distances = self._compute_pairwise_distances(updates)
        
        # Find outliers: nodes with high average distance
        avg_distances = [np.mean(distances[i]) for i in range(len(updates))]
        threshold = np.mean(avg_distances) + 1.5 * np.std(avg_distances)
        
        malicious_indices = [i for i, d in enumerate(avg_distances) if d > threshold]
        
        # Limit detections to tolerance level
        max_detections = max(1, int(len(updates) * self.tolerance))
        return malicious_indices[:max_detections]
    
    def filter_and_aggregate(self, updates: List[Gradient]) -> Gradient:
        """Filter Byzantine updates and return clean aggregate"""
        malicious_idx = self.detect_malicious_updates(updates)
        
        clean_updates = [u for i, u in enumerate(updates) if i not in malicious_idx]
        
        if not clean_updates:
            # Fallback: all marked as Byzantine, use median
            return self._geometric_median(updates)
        
        # Aggregate clean updates
        return self._mean_aggregate(clean_updates)
    
    def _compute_pairwise_distances(self, updates: List[Gradient]) -> np.ndarray:
        """L2 distance between all pairs"""
        n = len(updates)
        distances = np.zeros((n, n))
        for i in range(n):
            for j in range(i+1, n):
                distances[i][j] = distances[j][i] = np.linalg.norm(
                    updates[i].flatten() - updates[j].flatten()
                )
        return distances
```

---

## Convergence Validation

### Convergence Criteria
```
A model converges when:

1. Gradient magnitude stabilizes:
   || ‚àáL || < threshold (e.g., 1e-5)
   
2. Loss improvement slows:
   (L_{t-1} - L_t) / L_{t-1} < min_improvement (e.g., 0.001)
   
3. Validation accuracy plateaus:
   (accuracy_t - accuracy_{t-1}) < threshold (e.g., 0.001)
```

### Convergence Detection Algorithm
```python
class ConvergenceDetector:
    """Detects model convergence"""
    
    def __init__(self, window_size: int = 5, threshold: float = 0.001):
        self.window_size = window_size
        self.threshold = threshold
        self.loss_history = []
        self.accuracy_history = []
    
    def check_convergence(self, loss: float, accuracy: float) -> bool:
        """Check if model has converged"""
        
        # Need minimum history
        if len(self.loss_history) < self.window_size:
            return False
        
        # Check loss improvement
        recent_losses = self.loss_history[-self.window_size:]
        loss_improvement = (recent_losses[0] - recent_losses[-1]) / recent_losses[0]
        
        if loss_improvement < self.threshold:
            return True  # Converged
        
        # Check accuracy plateau
        recent_accs = self.accuracy_history[-self.window_size:]
        acc_improvement = recent_accs[-1] - recent_accs[0]
        
        if acc_improvement < self.threshold:
            return True  # Converged
        
        return False
```

---

## Learning Rate Scheduling

### Adaptive Learning Rate
```
Adjust learning rate based on convergence:

Œ±_t = Œ±_0 * schedule(t)

Schedule options:
1. Step decay: Œ± *= 0.1 every 10 rounds
2. Exponential: Œ± = Œ±_0 * exp(-t/œÑ)
3. Adaptive: Œ± = Œ±_0 / (1 + t)

For federated learning:
  - Start with higher Œ± (more aggressive)
  - Decrease as convergence approaches
  - Account for staleness (older updates = lower Œ±)
```

### Implementation
```python
class AdaptiveLearningRate:
    """Adaptive learning rate scheduler"""
    
    def __init__(self, initial_lr: float = 0.1, schedule: str = "step"):
        self.initial_lr = initial_lr
        self.schedule = schedule
        self.round_number = 0
    
    def get_lr(self, staleness: float = 0) -> float:
        """Get learning rate for current round
        
        Args:
            staleness: fraction indicating how stale updates are (0-1)
        """
        
        if self.schedule == "step":
            # Decay every 10 rounds
            decay_factor = 0.1 ** (self.round_number // 10)
        elif self.schedule == "exponential":
            decay_factor = np.exp(-self.round_number / 100)
        else:  # adaptive
            decay_factor = 1 / (1 + self.round_number)
        
        # Account for staleness: stale updates use lower LR
        staleness_factor = 1 - 0.5 * staleness
        
        return self.initial_lr * decay_factor * staleness_factor
```

---

## Integration with Mesh Network

### SPIFFE Identity Validation
```python
class FLUpdate:
    """Federated learning update with SPIFFE identity"""
    
    def __init__(self, node_id: str, gradient: np.ndarray, 
                 svid: str, signature: bytes):
        self.node_id = node_id
        self.gradient = gradient
        self.svid = svid  # SPIFFE identity
        self.signature = signature  # PQC signature
        self.timestamp = time.time()
    
    def validate(self, spiffe_controller) -> bool:
        """Validate update signature and identity"""
        
        # Verify SPIFFE identity is valid (not expired)
        if not spiffe_controller.validate_svid(self.svid):
            return False
        
        # Verify PQC signature
        message = self.gradient.tobytes() + self.node_id.encode()
        if not crypto.verify(message, self.signature, self.svid):
            return False
        
        return True
```

### Mesh Network Integration Points
```
1. Update Distribution:
   - Model broadcast to all nodes via mesh beacon
   - Uses PQC signature for integrity
   - SPIFFE identity for source verification

2. Update Collection:
   - Nodes send updates via mesh gossip
   - Updates validated against node's SPIFFE identity
   - Late updates detected and handled

3. Failure Handling:
   - Node failure ‚Üí MAPE-K loop detects
   - Failed node's pending updates discarded
   - Aggregation continues with healthy nodes
   - SPIFFE identity refresh on node recovery
```

---

## Performance Characteristics

### Throughput
```
1000 nodes, 1000-dim model:

Batch Async (K=850):
  - Updates per round: 850
  - Aggregation time: ~100ms
  - Round time: ~6 seconds
  - Throughput: 141 updates/sec

Hierarchical (10 zones):
  - Updates per zone: ~100
  - Zone aggregation: ~50ms each
  - Central aggregation: ~100ms
  - Round time: ~6 seconds
  - Throughput: same, but lower bandwidth

Streaming:
  - Incremental updates
  - Throughput: limited by network (e.g., 1000 updates/sec)
  - No rounds, continuous
```

### Scalability
```
Batch Async:
  - Works well up to 1000 nodes
  - Central becomes bottleneck at 5000+ nodes
  - Can handle 30% Byzantine

Hierarchical:
  - Works well up to 10,000 nodes
  - Edge aggregators scale linearly
  - Can handle 30% Byzantine per zone

Streaming:
  - No aggregation overhead
  - Works at any scale (bandwidth limited)
  - Byzantine: harder to detect without rounds
```

---

## Failure Modes & Recovery

### Node Failure
```
During training round:
  1. Node crashes mid-training
  2. MAPE-K detects failure
  3. Timeout on update from failed node
  4. Continue with healthy nodes
  5. Node recovers, rejoins
  6. Gets latest model via mesh beacon
  7. Resumes training

Recovery time: ~30 seconds
Impact: Single round loss (6 seconds)
Data loss: No (Byzantine filtering prevents bad updates)
```

### Network Partition
```
Scenario: 50-50 partition

Partition phase:
  - Nodes in partition A continue training
  - Nodes in partition B continue training
  - No central coordination
  - Models diverge

Healing (when partition heals):
  - Nodes exchange models
  - Central takes weighted average
  - Continue with merged model
  - Convergence may reset (depending on divergence)

Recovery time: ~60 seconds
Impact: Potential convergence reset
Mitigation: Consensus merge algorithm
```

### Byzantine Node Injection
```
Adversary sends bad update:
  1. Update arrives with valid SPIFFE signature
  2. Gradient quality check (L2 norm bounds)
  3. Byzantine detector flags as outlier
  4. Update excluded from aggregation
  5. Honest nodes' updates used

Detection rate: >95% (empirical)
False positive rate: <1%
Impact: Single round loss for attacker
```

---

## Deployment Checklist

- [ ] Load test with 100 ‚Üí 500 ‚Üí 1000 nodes (from Phase 5)
- [ ] Verify aggregation correctness (unit tests)
- [ ] Validate Byzantine filtering (chaos tests)
- [ ] Test failure recovery (fault injection tests)
- [ ] Measure convergence on real ML task
- [ ] Benchmark bandwidth usage
- [ ] Validate SPIFFE identity integration
- [ ] Document configuration for operators
- [ ] Create runbooks for common failures
- [ ] Deploy canary to 100 nodes
- [ ] Monitor convergence metrics
- [ ] Scale to production (1000+ nodes)

---

## Success Metrics

### Training
- ‚úÖ Convergence achieved within 100 rounds
- ‚úÖ Final accuracy matches centralized baseline ¬±1%
- ‚úÖ Training time < 1 hour for typical task

### Scaling
- ‚úÖ Latency per round < 10 seconds at 1000 nodes
- ‚úÖ Byzantine detection accuracy > 95%
- ‚úÖ Bandwidth usage < 100MB per round (with compression)

### Reliability
- ‚úÖ Recovery from node failure < 60 seconds
- ‚úÖ Zero data loss under Byzantine attack
- ‚úÖ Availability > 99.9%

---

**Document Status**: üìã Specification Complete  
**Next Steps**: Implementation and integration tests
