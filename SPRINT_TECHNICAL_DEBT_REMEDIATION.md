# üöÄ –°–ø—Ä–∏–Ω—Ç: –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –î–æ–ª–≥–∞ x0tta6bl4

**–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞:** 30 –Ω–æ—è–±—Ä—è 2025  
**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** 13-17 –Ω–µ–¥–µ–ª—å (528-704 —á–∞—Å–∞)  
**–¶–µ–ª—å:** –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–µ—Ä–æ–≤ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ 95% production readiness

---

## üìä –û–±–∑–æ—Ä –°–ø—Ä–∏–Ω—Ç–∞

| –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | Issues | –ß–∞—Å—ã | –ù–µ–¥–µ–ª–∏ | –°—Ç–∞—Ç—É—Å |
|-----------|--------|------|--------|--------|
| **P0 (–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ)** | 4 | 224-344h | 2-4 | üî¥ –ù–∞—á–∞—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ |
| **P1 (–í—ã—Å–æ–∫–∏–π)** | 4 | 184-240h | 3-4 | üü† –ü–æ—Å–ª–µ P0 |
| **P2 (–°—Ä–µ–¥–Ω–∏–π)** | 3 | 120h | 2-3 | üü° –ü–æ—Å–ª–µ P1 |
| **Total** | 11 | 528-704h | 13-17 | ‚úÖ Roadmap ready |

---

## üî¥ –§–ê–ó–ê 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ (P0) ‚Äî Weeks 1-4

### Week 1: Immediate Fixes (44-64 hours)

#### –ó–∞–¥–∞—á–∞ 1.1: Payment Verification ‚Äî USDT (TRC-20) ‚úÖ
**ID:** `sprint-p0-1`  
**–§–∞–π–ª:** `src/sales/telegram_bot.py:185-187`  
**–í—Ä–µ–º—è:** 12-20 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
```python
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
# requirements.txt: –¥–æ–±–∞–≤–∏—Ç—å httpx, tronpy

# 2. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å TronScan API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
async def check_usdt_payment(order_id: str, amount: int) -> bool:
    async with httpx.AsyncClient() as client:
        # –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∫–æ—à–µ–ª—å–∫–∞
        response = await client.get(
            f"https://api.trongrid.io/v1/accounts/{wallet_address}/transactions",
            params={
                "limit": 20,
                "only_confirmed": True,
                "only_to": True
            },
            timeout=10.0
        )
        
        transactions = response.json().get("data", [])
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞–∂–¥—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
        for tx in transactions:
            if verify_usdt_transaction(tx, amount, order_id):
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
                await save_payment_confirmation(order_id, tx["txID"])
                return True
        
        return False

def verify_usdt_transaction(tx: dict, amount: int, order_id: str) -> bool:
    """Verify USDT transaction matches order."""
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å contract address (USDT TRC-20)
    if tx.get("contract_address") != USDT_TRC20_CONTRACT:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å amount
    tx_amount = int(tx.get("parameter", {}).get("value", {}).get("amount", 0))
    if tx_amount != amount:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å timestamp (–Ω–µ —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞)
    tx_time = tx.get("block_timestamp", 0)
    if time.time() * 1000 - tx_time > 3600000:
        return False
    
    return True
```

**–¢–µ—Å—Ç—ã:**
- Unit —Ç–µ—Å—Ç—ã –¥–ª—è `verify_usdt_transaction()`
- Integration —Ç–µ—Å—Ç—ã —Å mock TronScan API
- E2E —Ç–µ—Å—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º –∫–æ—à–µ–ª—å–∫–æ–º (testnet)

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ USDT –ø–ª–∞—Ç–µ–∂–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ –¢–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç (100% coverage –¥–ª—è payment verification)
- ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫

---

#### –ó–∞–¥–∞—á–∞ 1.2: Payment Verification ‚Äî TON ‚úÖ
**ID:** `sprint-p0-2`  
**–§–∞–π–ª:** `src/sales/telegram_bot.py:192-193`  
**–í—Ä–µ–º—è:** 12-20 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
```python
async def check_ton_payment(order_id: str, amount: int) -> bool:
    async with httpx.AsyncClient() as client:
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TON API (tonapi.io –∏–ª–∏ toncenter.com)
        response = await client.get(
            f"https://tonapi.io/v2/accounts/{wallet_address}/transactions",
            params={
                "limit": 20,
                "min_lt": get_last_processed_lt(order_id)
            },
            headers={"Authorization": f"Bearer {TON_API_KEY}"},
            timeout=10.0
        )
        
        transactions = response.json().get("transactions", [])
        
        for tx in transactions:
            if verify_ton_transaction(tx, amount, order_id):
                await save_payment_confirmation(order_id, tx["hash"])
                return True
        
        return False
```

**–¢–µ—Å—Ç—ã:** –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ USDT

---

#### –ó–∞–¥–∞—á–∞ 1.3: Payment Verification ‚Äî Telegram Bot Integration ‚úÖ
**ID:** `sprint-p0-3`  
**–§–∞–π–ª:** `src/sales/telegram_bot.py`  
**–í—Ä–µ–º—è:** 8-12 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
- –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –ø–ª–∞—Ç–µ–∂–µ–π –≤ bot handlers
- –î–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—Ç–ø—Ä–∞–≤–∫—É download links –ø–æ—Å–ª–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
- –î–æ–±–∞–≤–∏—Ç—å retry logic –¥–ª—è failed API calls
- –î–æ–±–∞–≤–∏—Ç—å rate limiting –¥–ª—è API calls

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ Bot –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–ª–∞—Ç–µ–∂–∏ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
- ‚úÖ Download links –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
- ‚úÖ –û—à–∏–±–∫–∏ API –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è gracefully

---

#### –ó–∞–¥–∞—á–∞ 1.4: Async Bottlenecks ‚Äî mesh_router.start() ‚úÖ
**ID:** `sprint-p0-4`  
**–§–∞–π–ª:** `src/core/app.py:145`  
**–í—Ä–µ–º—è:** 4-6 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
```python
# ‚ùå –¢–ï–ö–£–©–ï–ï
@app.on_event("startup")
async def startup_event():
    await mesh_sync.start()
    mesh_router.start()  # ‚Üê –ë–õ–û–ö–ò–†–£–ï–¢ event loop

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û
@app.on_event("startup")
async def startup_event():
    await mesh_sync.start()
    # Off-thread execution
    await asyncio.to_thread(mesh_router.start)
```

**–¢–µ—Å—Ç—ã:**
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ startup –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç
- Load test: 1000 concurrent requests –≤–æ –≤—Ä–µ–º—è startup

---

#### –ó–∞–¥–∞—á–∞ 1.5: Async Bottlenecks ‚Äî train_model_background() ‚úÖ
**ID:** `sprint-p0-5`  
**–§–∞–π–ª:** `src/core/app.py:147-151`  
**–í—Ä–µ–º—è:** 4-6 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
```python
# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û
async def train_model_async():
    await asyncio.to_thread(train_model_background)
asyncio.create_task(train_model_async())
```

---

#### –ó–∞–¥–∞—á–∞ 1.6: Async Bottlenecks ‚Äî Load Testing ‚úÖ
**ID:** `sprint-p0-6`  
**–í—Ä–µ–º—è:** 8-12 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
- –°–æ–∑–¥–∞—Ç—å load test —Å–∫—Ä–∏–ø—Ç (k6 –∏–ª–∏ locust)
- –ò–∑–º–µ—Ä–∏—Ç—å throughput –¥–æ/–ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
- –ò–∑–º–µ—Ä–∏—Ç—å latency p50/p95/p99 –¥–æ/–ø–æ—Å–ª–µ
- –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ Throughput: 6,800+ msg/sec (—Ü–µ–ª—å)
- ‚úÖ Latency p95: <100ms (—Ü–µ–ª—å)
- ‚úÖ –ù–µ—Ç blocking –≤ event loop

---

### Week 2-4: Core Functionality (180-280 hours)

#### –ó–∞–¥–∞—á–∞ 2.1-2.3: eBPF Observability ‚úÖ
**ID:** `sprint-p0-7`, `sprint-p0-8`, `sprint-p0-9`  
**–§–∞–π–ª:** `src/network/ebpf/loader.py`  
**–í—Ä–µ–º—è:** 120-180 —á–∞—Å–æ–≤ (40-60h –Ω–∞ –∑–∞–¥–∞—á—É)

**–î–µ–π—Å—Ç–≤–∏—è:**

**2.1: attach_to_interface()**
```python
def attach_to_interface(self, program_id: str, interface: str):
    """Attach eBPF program to network interface."""
    # 1. Verify interface exists
    interface_path = Path(f"/sys/class/net/{interface}")
    if not interface_path.exists():
        raise EBPFAttachError(f"Interface not found: {interface}")
    
    # 2. Check interface is up
    operstate = (interface_path / "operstate").read_text().strip()
    if operstate != "up":
        raise EBPFAttachError(f"Interface not up: {interface}")
    
    # 3. Load program (if not already loaded)
    program_info = self.loaded_programs[program_id]
    program_path = program_info["path"]
    
    # 4. Attach via bpftool
    # Try XDP HW mode first, fallback to DRV, then SKB
    for mode in ["xdp", "xdpgeneric", "xdpoffload"]:
        try:
            result = subprocess.run(
                ["bpftool", "net", "attach", "xdp", "id", str(program_info["fd"]), "dev", interface],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                logger.info(f"‚úÖ Attached {program_id} to {interface} in {mode} mode")
                return
        except subprocess.TimeoutExpired:
            continue
    
    raise EBPFAttachError(f"Failed to attach {program_id} to {interface}")
```

**2.2: detach_from_interface()**
```python
def detach_from_interface(self, program_id: str, interface: str):
    """Detach eBPF program from network interface."""
    # 1. Verify program is attached
    result = subprocess.run(
        ["bpftool", "net", "show", "dev", interface],
        capture_output=True,
        text=True,
        timeout=5
    )
    
    if program_id not in result.stdout:
        logger.warning(f"Program {program_id} not attached to {interface}")
        return
    
    # 2. Detach
    subprocess.run(
        ["bpftool", "net", "detach", "xdp", "dev", interface],
        capture_output=True,
        timeout=5
    )
    
    # 3. Verify detachment
    result = subprocess.run(
        ["bpftool", "net", "show", "dev", interface],
        capture_output=True,
        text=True,
        timeout=5
    )
    
    if program_id not in result.stdout:
        logger.info(f"‚úÖ Detached {program_id} from {interface}")
    else:
        raise EBPFAttachError(f"Failed to detach {program_id} from {interface}")
```

**2.3: XDP Mode Negotiation**
```python
def _negotiate_xdp_mode(self, interface: str) -> str:
    """Negotiate best XDP mode (HW ‚Üí DRV ‚Üí SKB)."""
    # Check driver support
    driver_path = Path(f"/sys/class/net/{interface}/device/driver")
    if driver_path.exists():
        driver = driver_path.resolve().name
        if driver in ["ixgbe", "i40e", "mlx5_core"]:
            return "xdp"  # Hardware offload
    
    # Check generic XDP support
    if Path(f"/sys/class/net/{interface}/xdp").exists():
        return "xdpgeneric"  # Generic mode
    
    return "xdpoffload"  # Fallback
```

**–¢–µ—Å—Ç—ã:**
- Unit —Ç–µ—Å—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
- Integration —Ç–µ—Å—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º (loopback)
- E2E —Ç–µ—Å—Ç—ã —Å XDP program

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ü—Ä–æ–≥—Ä–∞–º–º—ã attach/detach —Ä–∞–±–æ—Ç–∞—é—Ç
- ‚úÖ XDP mode negotiation —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ –¢–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç (100% coverage)

---

#### –ó–∞–¥–∞—á–∞ 2.4-2.6: GraphSAGE Causal Analysis ‚úÖ
**ID:** `sprint-p0-10`, `sprint-p0-11`, `sprint-p0-12`  
**–§–∞–π–ª:** `src/ml/causal_analysis.py`  
**–í—Ä–µ–º—è:** 60-100 —á–∞—Å–æ–≤ (20-35h –Ω–∞ –∑–∞–¥–∞—á—É)

**–î–µ–π—Å—Ç–≤–∏—è:**

**2.4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å GraphSAGE**
```python
# src/ml/graphsage_anomaly_detector.py
def predict_with_causal(self, node_id: str, node_features: Dict) -> Tuple[AnomalyPrediction, CausalAnalysis]:
    """Predict anomaly and provide causal analysis."""
    # 1. GraphSAGE prediction
    prediction = self.predict(node_id, node_features)
    
    # 2. Causal analysis if anomaly detected
    if prediction.is_anomaly:
        causal = self.causal_analyzer.analyze(node_id, node_features, prediction)
        return prediction, causal
    
    return prediction, None
```

**2.5: SHAP Values**
```python
# –î–æ–±–∞–≤–∏—Ç—å SHAP –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
import shap

def explain_anomaly(self, node_features: Dict, model_output: float) -> Dict[str, float]:
    """Explain anomaly using SHAP values."""
    explainer = shap.TreeExplainer(self.model)
    shap_values = explainer.shap_values(node_features)
    
    return {
        feature: float(shap_value)
        for feature, shap_value in zip(node_features.keys(), shap_values)
    }
```

**2.6: Root Cause Detection**
```python
def detect_root_cause(self, incident: IncidentEvent, graph: nx.Graph) -> List[RootCause]:
    """Detect root cause of incident."""
    # 1. Build causal graph
    causal_graph = self.build_causal_graph(incident, graph)
    
    # 2. Find root nodes (no incoming edges)
    root_nodes = [n for n in causal_graph.nodes() if causal_graph.in_degree(n) == 0]
    
    # 3. Score root causes
    root_causes = []
    for root in root_nodes:
        score = self._calculate_root_cause_score(root, causal_graph, incident)
        root_causes.append(RootCause(
            node_id=root,
            score=score,
            confidence=self._calculate_confidence(root, causal_graph)
        ))
    
    return sorted(root_causes, key=lambda x: x.score, reverse=True)
```

**–¢–µ—Å—Ç—ã:**
- Unit —Ç–µ—Å—Ç—ã –¥–ª—è causal analysis
- Integration —Ç–µ—Å—Ç—ã —Å GraphSAGE
- E2E —Ç–µ—Å—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞–º–∏

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- ‚úÖ Causal analysis –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω —Å GraphSAGE
- ‚úÖ SHAP values –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
- ‚úÖ Root cause detection —Ä–∞–±–æ—Ç–∞–µ—Ç —Å >90% accuracy

---

## üü† –§–ê–ó–ê 2: –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (P1) ‚Äî Weeks 5-8

### Week 5-6: SPIFFE & Canary (80 hours)

#### –ó–∞–¥–∞—á–∞ 3.1-3.2: SPIFFE Auto-Renew ‚úÖ
**ID:** `sprint-p1-1`, `sprint-p1-2`  
**–§–∞–π–ª:** `src/security/spiffe/workload/api_client_production.py:229-232`  
**–í—Ä–µ–º—è:** 40 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
```python
async def auto_renew_svid(self, renewal_threshold: float = 0.5) -> None:
    """Auto-renew SVID when threshold is reached."""
    while True:
        try:
            if self.current_svid:
                # Calculate remaining time
                now = time.time()
                remaining = self.current_svid.expiry - now
                total_ttl = self.current_svid.expiry - self.current_svid.issued_at
                threshold_time = total_ttl * renewal_threshold
                
                # Renew if below threshold
                if remaining < threshold_time:
                    logger.info(f"üîÑ Auto-renewing X.509-SVID (remaining: {remaining:.0f}s)")
                    new_svid = await self.fetch_x509_svid()
                    
                    if new_svid:
                        self.current_svid = new_svid
                        logger.info(f"‚úÖ SVID renewed, new expiry: {new_svid.expiry}")
                    else:
                        logger.error("‚ùå Failed to renew SVID")
            
            # Check every 5 minutes
            await asyncio.sleep(300)
            
        except Exception as e:
            logger.error(f"‚ùå Auto-renew error: {e}")
            await asyncio.sleep(60)  # Retry after 1 minute
```

**–¢–µ—Å—Ç—ã:**
- Unit —Ç–µ—Å—Ç—ã –¥–ª—è auto-renew logic
- Integration —Ç–µ—Å—Ç—ã —Å mock SPIRE server
- E2E —Ç–µ—Å—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º SPIRE (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)

---

#### –ó–∞–¥–∞—á–∞ 3.3-3.4: Canary Deployment ‚úÖ
**ID:** `sprint-p1-6`, `sprint-p1-7`  
**–§–∞–π–ª:** `src/deployment/canary_deployment.py:189-190`  
**–í—Ä–µ–º—è:** 40 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
```python
def rollback(self) -> bool:
    """Rollback canary deployment."""
    # 1. Check metrics
    error_rate = self.get_error_rate()
    latency_p95 = self.get_latency_p95()
    
    # 2. Rollback if metrics exceed thresholds
    if error_rate > 0.05 or latency_p95 > 200:
        logger.warning(f"üö® Canary rollback triggered: error_rate={error_rate}, latency={latency_p95}")
        
        # 3. Scale down canary
        self.scale_down_canary()
        
        # 4. Scale up stable version
        self.scale_up_stable()
        
        # 5. Notify team
        await self.send_rollback_notification(error_rate, latency_p95)
        
        return True
    
    return False
```

---

### Week 7-8: Deployment Automation (80-120 hours)

#### –ó–∞–¥–∞—á–∞ 4.1-4.3: Cloud Deployment ‚úÖ
**ID:** `sprint-p1-3`, `sprint-p1-4`, `sprint-p1-5`  
**–§–∞–π–ª:** `staging/deploy_staging.sh`  
**–í—Ä–µ–º—è:** 80-120 —á–∞—Å–æ–≤ (27-40h –Ω–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞)

**–î–µ–π—Å—Ç–≤–∏—è:**

**4.1: AWS Deployment**
```bash
# staging/deploy_staging.sh
deploy_aws() {
    log_info "Deploying to AWS..."
    
    # 1. Login to ECR
    aws ecr get-login-password --region $AWS_REGION | \
        docker login --username AWS --password-stdin $AWS_ECR_REGISTRY
    
    # 2. Build and push image
    docker build -f Dockerfile.app -t $AWS_ECR_REGISTRY/x0tta6bl4:$VERSION .
    docker push $AWS_ECR_REGISTRY/x0tta6bl4:$VERSION
    
    # 3. Update ECS service
    aws ecs update-service \
        --cluster $AWS_ECS_CLUSTER \
        --service $AWS_ECS_SERVICE \
        --force-new-deployment \
        --region $AWS_REGION
    
    log_info "‚úÖ AWS deployment complete"
}
```

**4.2: Azure Deployment**
```bash
deploy_azure() {
    log_info "Deploying to Azure..."
    
    # 1. Login to ACR
    az acr login --name $AZURE_ACR_NAME
    
    # 2. Build and push
    docker build -f Dockerfile.app -t $AZURE_ACR_NAME.azurecr.io/x0tta6bl4:$VERSION .
    docker push $AZURE_ACR_NAME.azurecr.io/x0tta6bl4:$VERSION
    
    # 3. Update AKS deployment
    az aks get-credentials --resource-group $AZURE_RG --name $AZURE_AKS_CLUSTER
    kubectl set image deployment/x0tta6bl4 x0tta6bl4=$AZURE_ACR_NAME.azurecr.io/x0tta6bl4:$VERSION
    
    log_info "‚úÖ Azure deployment complete"
}
```

**4.3: GCP Deployment**
```bash
deploy_gcp() {
    log_info "Deploying to GCP..."
    
    # 1. Configure gcloud
    gcloud auth configure-docker $GCP_REGION-docker.pkg.dev
    
    # 2. Build and push
    docker build -f Dockerfile.app -t $GCP_REGION-docker.pkg.dev/$GCP_PROJECT/$GCP_REPO/x0tta6bl4:$VERSION .
    docker push $GCP_REGION-docker.pkg.dev/$GCP_PROJECT/$GCP_REPO/x0tta6bl4:$VERSION
    
    # 3. Update GKE deployment
    gcloud container clusters get-credentials $GCP_GKE_CLUSTER --region $GCP_REGION
    kubectl set image deployment/x0tta6bl4 x0tta6bl4=$GCP_REGION-docker.pkg.dev/$GCP_PROJECT/$GCP_REPO/x0tta6bl4:$VERSION
    
    log_info "‚úÖ GCP deployment complete"
}
```

---

### Week 8: Alerting System (24-40 hours)

#### –ó–∞–¥–∞—á–∞ 5.1-5.3: Alerting Integration ‚úÖ
**ID:** `sprint-p1-8`, `sprint-p1-9`, `sprint-p1-10`  
**–§–∞–π–ª:** `src/monitoring/pqc_metrics.py`  
**–í—Ä–µ–º—è:** 24-40 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**

**5.1: Prometheus Alertmanager**
```python
# src/monitoring/alerting.py
class AlertManager:
    def __init__(self, alertmanager_url: str):
        self.alertmanager_url = alertmanager_url
        self.client = httpx.AsyncClient()
    
    async def send_alert(self, alert_name: str, severity: str, message: str, labels: Dict = None):
        """Send alert to Prometheus Alertmanager."""
        alert = {
            "labels": {
                "alertname": alert_name,
                "severity": severity,
                "service": "x0tta6bl4",
                **(labels or {})
            },
            "annotations": {
                "summary": message,
                "description": f"x0tta6bl4 alert: {message}"
            }
        }
        
        await self.client.post(
            f"{self.alertmanager_url}/api/v1/alerts",
            json=[alert],
            timeout=5.0
        )
```

**5.2: Telegram Notifications**
```python
async def send_telegram_alert(self, message: str, severity: str):
    """Send alert to Telegram."""
    from telegram import Bot
    
    bot = Bot(token=TELEGRAM_BOT_TOKEN)
    await bot.send_message(
        chat_id=TELEGRAM_ALERT_CHAT_ID,
        text=f"üö® [{severity.upper()}] {message}",
        parse_mode="Markdown"
    )
```

**5.3: PagerDuty Integration**
```python
async def send_pagerduty_alert(self, message: str, severity: str):
    """Send alert to PagerDuty."""
    event = {
        "routing_key": PAGERDUTY_INTEGRATION_KEY,
        "event_action": "trigger",
        "payload": {
            "summary": message,
            "severity": severity,
            "source": "x0tta6bl4"
        }
    }
    
    await httpx.post("https://events.pagerduty.com/v2/enqueue", json=event)
```

**–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:**
```python
# src/monitoring/pqc_metrics.py
async def on_pqc_handshake_failure(reason: str):
    """Handle PQC handshake failure."""
    await alert_manager.send_alert(
        "PQC_HANDSHAKE_FAILURE",
        "critical",
        f"PQC handshake failed: {reason}",
        {"reason": reason}
    )
    
    # Also send to Telegram
    await alert_manager.send_telegram_alert(
        f"PQC handshake failed: {reason}",
        "critical"
    )
```

---

## üü° –§–ê–ó–ê 3: –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (P2) ‚Äî Weeks 9-11

### Week 9: Digital Twin (40 hours)

#### –ó–∞–¥–∞—á–∞ 6.1: Digital Twin ‚Äî links_affected ‚úÖ
**ID:** `sprint-p2-1`  
**–§–∞–π–ª:** `src/simulation/digital_twin.py:605`  
**–í—Ä–µ–º—è:** 40 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**
```python
def simulate_node_failure(self, node_ids: List[str]) -> SimulationResult:
    """Simulate node failures and calculate impact."""
    failed_nodes = set(node_ids)
    
    # Calculate links affected
    links_affected = 0
    for node_id in failed_nodes:
        # Get all links connected to this node
        node_links = [
            link for link in self.links
            if link.source == node_id or link.target == node_id
        ]
        links_affected += len(node_links)
    
    # Remove duplicate links (bidirectional)
    links_affected = links_affected // 2
    
    # Calculate connectivity
    remaining_nodes = set(self.nodes.keys()) - failed_nodes
    connectivity_maintained = self._calculate_connectivity(remaining_nodes)
    
    return SimulationResult(
        failed_nodes=list(failed_nodes),
        links_affected=links_affected,  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û
        connectivity_maintained=connectivity_maintained,
        packet_loss_total=0.1 * len(failed_nodes)
    )
```

---

### Week 10: Code Consolidation (40 hours)

#### –ó–∞–¥–∞—á–∞ 7.1-7.2: Code Consolidation ‚úÖ
**ID:** `sprint-p2-2`, `sprint-p2-3`  
**–í—Ä–µ–º—è:** 40 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**

**7.1: Feature Flags**
```python
# src/core/feature_flags.py
class FeatureFlags:
    BYZANTINE_PROTECTION = os.getenv("X0TTA6BL4_BYZANTINE", "false").lower() == "true"
    FAILOVER_ENABLED = os.getenv("X0TTA6BL4_FAILOVER", "false").lower() == "true"
    PQC_BEACONS = os.getenv("X0TTA6BL4_PQC_BEACONS", "false").lower() == "true"
    MINIMAL_MODE = os.getenv("X0TTA6BL4_MINIMAL", "false").lower() == "true"
```

**7.2: Consolidated App**
```python
# src/core/app.py (–∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
@app.on_event("startup")
async def startup_event():
    # Byzantine protection (if enabled)
    if FeatureFlags.BYZANTINE_PROTECTION:
        await setup_byzantine_protection()
    
    # Failover (if enabled)
    if FeatureFlags.FAILOVER_ENABLED:
        await setup_failover()
    
    # PQC beacons (if enabled)
    if FeatureFlags.PQC_BEACONS:
        await setup_pqc_beacons()
    
    # Minimal mode (if enabled)
    if FeatureFlags.MINIMAL_MODE:
        await setup_minimal_mode()
    
    # Core functionality (always)
    await mesh_sync.start()
    await asyncio.to_thread(mesh_router.start)
```

---

### Week 11: Error Handling (40 hours)

#### –ó–∞–¥–∞—á–∞ 8.1-8.2: Error Handling Framework ‚úÖ
**ID:** `sprint-p2-4`, `sprint-p2-5`  
**–í—Ä–µ–º—è:** 40 —á–∞—Å–æ–≤

**–î–µ–π—Å—Ç–≤–∏—è:**

**8.1: ErrorHandler Framework**
```python
# src/core/error_handler.py
class ErrorHandler:
    """Unified error handling framework."""
    
    @staticmethod
    async def handle_error(error: Exception, context: str, severity: str = "error"):
        """Handle error with consistent logging and alerting."""
        # 1. Structured logging
        logger.error(
            f"Error in {context}",
            extra={
                "error_type": type(error).__name__,
                "error_message": str(error),
                "severity": severity,
                "context": context,
                "traceback": traceback.format_exc()
            }
        )
        
        # 2. Alert if critical
        if severity == "critical":
            await alert_manager.send_alert(
                f"ERROR_{context.upper()}",
                "critical",
                f"Critical error in {context}: {error}",
                {"error_type": type(error).__name__}
            )
        
        # 3. Metrics
        error_counter.labels(
            error_type=type(error).__name__,
            context=context,
            severity=severity
        ).inc()
```

**8.2: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è**
- –ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ `except Exception: pass` –Ω–∞ `ErrorHandler.handle_error()`
- –î–æ–±–∞–≤–∏—Ç—å structured logging –≤–æ –≤—Å–µ error handlers
- –î–æ–±–∞–≤–∏—Ç—å metrics –¥–ª—è –≤—Å–µ—Ö –æ—à–∏–±–æ–∫

---

## üìÖ Timeline

```
Week 1:   Payment Verification + Async Fixes (44-64h)
Week 2-4: eBPF + GraphSAGE (180-280h)
Week 5-6: SPIFFE + Canary (80h)
Week 7-8: Cloud Deployment + Alerting (104-160h)
Week 9:   Digital Twin (40h)
Week 10:  Code Consolidation (40h)
Week 11:  Error Handling (40h)

Total: 528-704 hours (13-17 weeks)
```

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –£—Å–ø–µ—Ö–∞

### Phase 1 (Week 4)
- ‚úÖ Payment verification —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- ‚úÖ Async bottlenecks —É—Å—Ç—Ä–∞–Ω–µ–Ω—ã (throughput 6,800+ msg/sec)
- ‚úÖ eBPF observability —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ GraphSAGE causal analysis —Ä–∞–±–æ—Ç–∞–µ—Ç

### Phase 2 (Week 8)
- ‚úÖ SPIFFE auto-renew —Ä–∞–±–æ—Ç–∞–µ—Ç
- ‚úÖ Cloud deployment –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω
- ‚úÖ Canary deployment —Å auto-rollback
- ‚úÖ Alerting —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç

### Phase 3 (Week 11)
- ‚úÖ Digital Twin –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω
- ‚úÖ Code consolidated —Å feature flags
- ‚úÖ Error handling —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω

### Final (Week 13-17)
- ‚úÖ TDR: 30.5% ‚Üí 8%
- ‚úÖ Production Ready: 60% ‚Üí 95%
- ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –ü—Ä–æ–≥—Ä–µ—Å—Å–∞

| –ù–µ–¥–µ–ª—è | –ó–∞–≤–µ—Ä—à–µ–Ω–æ | –û—Å—Ç–∞–ª–æ—Å—å | TDR | Production Ready |
|--------|-----------|----------|-----|------------------|
| **Week 1** | Payment + Async | eBPF + GraphSAGE | 30.5% | 60% |
| **Week 4** | P0 Complete | P1 + P2 | 20% | 75% |
| **Week 8** | P0 + P1 Complete | P2 | 12% | 90% |
| **Week 11** | All Complete | Polish | 8% | 95% |

---

## üöÄ –ù–∞—á–∞–ª–æ –°–ø—Ä–∏–Ω—Ç–∞

**–ü–µ—Ä–≤—ã–µ —à–∞–≥–∏ (Week 1, Day 1):**

1. **Setup (2h)**
   - –°–æ–∑–¥–∞—Ç—å feature branch: `sprint/technical-debt-remediation`
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å tracking board (GitHub Projects / Jira)
   - –°–æ–∑–¥–∞—Ç—å milestone: "Technical Debt Remediation"

2. **Payment Verification (Day 1-2)**
   - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (httpx, tronpy)
   - –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å TronScan API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
   - –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã

3. **Async Fixes (Day 2-3)**
   - –û–±–µ—Ä–Ω—É—Ç—å blocking calls –≤ `asyncio.to_thread()`
   - Load testing
   - –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–∏—è

---

**Status:** ‚úÖ –°–ø—Ä–∏–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É  
**Start:** Week 1, Day 1  
**Finish:** Week 13-17 (Mid-Q2 2026)

