# üîó OLLAMA_INTEGRATION_GUIDE.md - –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

**–ü–æ–ª–Ω–∞—è –ø–æ—à–∞–≥–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Ollama –≤ –ë–∞–∑–∏—Å-–í–µ–±**  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–û–¢–û–í–´–ô –ö–û–î, COPY-PASTE  
**–î–∞—Ç–∞:** 17 —è–Ω–≤–∞—Ä—è 2026

---

## üìã –°–û–î–ï–†–ñ–ê–ù–ò–ï

1. [–≠—Ç–∞–ø 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama](#—ç—Ç–∞–ø-1-—É—Å—Ç–∞–Ω–æ–≤–∫–∞-ollama)
2. [–≠—Ç–∞–ø 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π](#—ç—Ç–∞–ø-2-–∑–∞–≥—Ä—É–∑–∫–∞-–º–æ–¥–µ–ª–µ–π)
3. [–≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ ollamaService.ts](#—ç—Ç–∞–ø-3-—Å–æ–∑–¥–∞–Ω–∏–µ-ollamaservicets)
4. [–≠—Ç–∞–ø 4: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ geminiService.ts](#—ç—Ç–∞–ø-4-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ-geminiserviects)
5. [–≠—Ç–∞–ø 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ AIAssistant.tsx](#—ç—Ç–∞–ø-5-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ-aiassistanttsx)
6. [–≠—Ç–∞–ø 6: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ](#—ç—Ç–∞–ø-6-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
7. [Troubleshooting](#troubleshooting)

---

## –≠—Ç–∞–ø 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

### –ù–∞ Linux (Ubuntu 22.04 LTS) - –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø

```bash
# –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —ç—Ç–∏ –∫–æ–º–∞–Ω–¥—ã –ø–æ –æ—á–µ—Ä–µ–¥–∏

# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama
curl https://ollama.ai/install.sh | sh

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
ollama --version
# –î–æ–ª–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞: ollama version is 0.1.x

# 3. –ó–∞–ø—É—Å–∫ Ollama –∫–∞–∫ —Å–∏—Å—Ç–µ–º–Ω—ã–π —Å–µ—Ä–≤–∏—Å
sudo systemctl start ollama
sudo systemctl enable ollama  # –ê–≤—Ç–æ–∑–∞–ø—É—Å–∫ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∑–∞–ø—É—â–µ–Ω
curl http://localhost:11434/api/tags
# {"models":[]} - –ø—É—Å—Ç–æ, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ

# 5. –ï—Å–ª–∏ –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è, –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
ollama serve
# –î–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å: Listening on 127.0.0.1:11434 (http)
```

### –ù–∞ macOS

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama —á–µ—Ä–µ–∑ Homebrew
brew install ollama

# 2. –ó–∞–ø—É—Å–∫ (–æ—Ç–∫—Ä–æ–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ)
ollama serve

# 3. –í –Ω–æ–≤–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–º –æ–∫–Ω–µ:
ollama pull qwen2.5:32b-instruct-q5_0
ollama pull mistral:7b-instruct-q5_0

# ‚úÖ –ì–æ—Ç–æ–≤–æ
```

### –ù–∞ Windows —á–µ—Ä–µ–∑ WSL2

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å WSL2: https://docs.microsoft.com/en-us/windows/wsl/install
# 2. –í —Ç–µ—Ä–º–∏–Ω–∞–ª–µ WSL2:
curl https://ollama.ai/install.sh | sh

# 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –Ω–∞ Linux)
ollama pull qwen2.5:32b-instruct-q5_0
ollama pull mistral:7b-instruct-q5_0
```

### –ù–∞ Windows (Native, –±–µ–∑ WSL2)

```bash
# 1. –°–∫–∞—á–∞—Ç—å —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫: https://ollama.ai/download/windows
# 2. –ó–∞–ø—É—Å—Ç–∏—Ç—å installer (.exe —Ñ–∞–π–ª)
# 3. –û—Ç–∫—Ä—ã—Ç—å PowerShell –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å:
ollama pull qwen2.5:32b-instruct-q5_0
ollama pull mistral:7b-instruct-q5_0
```

---

## –≠—Ç–∞–ø 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π

```bash
# ‚ö†Ô∏è –í–ê–ñ–ù–û: –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π!

# –ú–æ–¥–µ–ª—å 1: Qwen 32B (–æ—Å–Ω–æ–≤–Ω–∞—è, –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
ollama pull qwen2.5:32b-instruct-q5_0
# –†–∞–∑–º–µ—Ä: ~12GB
# –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: 10-15 –º–∏–Ω—É—Ç (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)

# –ú–æ–¥–µ–ª—å 2: Mistral 14B (–±—ã—Å—Ç—Ä–∞—è, –¥–ª—è —á–∞—Ç–∞)
ollama pull mistral:7b-instruct-q5_0
# –†–∞–∑–º–µ—Ä: ~4GB
# –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: 5-10 –º–∏–Ω—É—Ç

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
ollama list
# –î–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å –æ–±–µ –º–æ–¥–µ–ª–∏ –≤ —Å–ø–∏—Å–∫–µ:
# NAME                                    ID              SIZE      MODIFIED
# qwen2.5:32b-instruct-q5_0              a1234b5c6d...  12 GB     2 minutes ago
# mistral:7b-instruct-q5_0               b2345c6d7e...  4.0 GB    1 minute ago

# ‚úÖ –ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!
```

---

## –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ ollamaService.ts

**–°–∫–æ–ø–∏—Ä—É–π—Ç–µ –≤–µ—Å—å –∫–æ–¥ –Ω–∏–∂–µ –≤ —Ñ–∞–π–ª `/services/ollamaService.ts`:**

```typescript
// /services/ollamaService.ts
// –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Ollama –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π

import type { CabinetConfig } from "../types";

interface OllamaResponse {
  model: string;
  created_at: string;
  response: string;
  done: boolean;
  context?: number[];
  total_duration: number;
  load_duration: number;
  prompt_eval_count: number;
  prompt_eval_duration: number;
  eval_count: number;
  eval_duration: number;
}

interface OllamaChatResponse {
  model: string;
  created_at: string;
  message: {
    role: string;
    content: string;
  };
  done: boolean;
  total_duration: number;
  load_duration: number;
  prompt_eval_count: number;
  prompt_eval_duration: number;
  eval_count: number;
  eval_duration: number;
}

/**
 * Ollama Local LLM Service
 * –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –æ–±–ª–∞—á–Ω–æ–≥–æ API
 * 
 * –ú–æ–¥–µ–ª–∏:
 * - qwen2.5:32b-instruct-q5_0 - –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π (97% —Ç–æ—á–Ω–æ—Å—Ç—å)
 * - mistral:7b-instruct-q5_0 - –¥–ª—è —á–∞—Ç–∞ –∏ –±—ã—Å—Ç—Ä—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
 * 
 * –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
 * - Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –∑–∞–ø—É—â–µ–Ω –Ω–∞ localhost:11434
 * - –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —á–µ—Ä–µ–∑ `ollama pull qwen...` –∏ `ollama pull mistral...`
 */
export class OllamaService {
  private ollamaUrl = "http://localhost:11434/api";
  private analysisModel = "qwen2.5:32b-instruct-q5_0"; // –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Ç–æ—á–Ω–µ–µ)
  private chatModel = "mistral:7b-instruct-q5_0"; // –î–ª—è —á–∞—Ç–∞ (–±—ã—Å—Ç—Ä–µ–µ)
  private isHealthy = false;

  constructor() {
    this.checkHealth();
  }

  /**
   * –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama —Å–µ—Ä–≤–µ—Ä–∞
   */
  private async checkHealth(): Promise<boolean> {
    try {
      const response = await fetch(`${this.ollamaUrl}/tags`);
      this.isHealthy = response.ok;
      return this.isHealthy;
    } catch (error) {
      console.error("Ollama server is not available:", error);
      this.isHealthy = false;
      return false;
    }
  }

  /**
   * –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞
   */
  public getHealth(): boolean {
    return this.isHealthy;
  }

  /**
   * –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —à–∫–∞—Ñ–∞ (–∏—Å–ø–æ–ª—å–∑—É—è Qwen 32B –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏)
   * 
   * @param cabinet - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —à–∫–∞—Ñ–∞
   * @returns –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
   */
  public async analyzeConstruction(cabinet: CabinetConfig): Promise<string> {
    const prompt = this.buildAnalysisPrompt(cabinet);
    return this.callOllama(this.analysisModel, prompt);
  }

  /**
   * –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º
   * 
   * @param cabinet - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
   * @returns –û—Ç—á–µ—Ç –æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º
   */
  public async conductTechnicalAudit(cabinet: CabinetConfig): Promise<string> {
    const prompt = `
    –¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤—É –º–µ–±–µ–ª–∏. –ü—Ä–æ–≤–µ–¥–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞:
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - –®–∏—Ä–∏–Ω–∞: ${cabinet.width}mm
    - –í—ã—Å–æ—Ç–∞: ${cabinet.height}mm
    - –ì–ª—É–±–∏–Ω–∞: ${cabinet.depth}mm
    - –ú–∞—Ç–µ—Ä–∏–∞–ª: ${cabinet.material || "–î–°–ü"}
    - –¢–æ–ª—â–∏–Ω–∞ —Å—Ç–µ–Ω–æ–∫: ${cabinet.wallThickness || "18"}mm
    - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: ${JSON.stringify(cabinet.shelves || [])}
    
    –ü—Ä–æ–≤–µ—Ä—å:
    1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –º–µ–±–µ–ª—å–Ω–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ (–ì–û–°–¢)
    2. –ü—Ä–æ—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –Ω–∞–≥—Ä—É–∑–æ–∫
    3. –ü—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤
    4. –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
    5. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ
    
    –§–æ—Ä–º–∞—Ç: –î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å:
    - ‚úÖ –ß—Ç–æ —Ö–æ—Ä–æ—à–æ
    - ‚ö†Ô∏è –ß—Ç–æ –Ω—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å
    - üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    `;

    return this.callOllama(this.analysisModel, prompt);
  }

  /**
   * –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç —ç–∫—Å–ø–µ—Ä—Ç–∞ –º–µ–±–µ–ª—å–Ω–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞
   * 
   * @param question - –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
   * @param context - –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
   * @returns –û—Ç–≤–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–∞
   */
  public async askFurnitureExpert(
    question: string,
    context?: string
  ): Promise<string> {
    const prompt = `
    –¢—ã - –æ–ø—ã—Ç–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–∏–∑–∞–π–Ω—É –∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤—É –º–µ–±–µ–ª–∏ —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º.
    
    ${context ? `–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞: ${context}` : ""}
    
    –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: ${question}
    
    –û—Ç–≤–µ—Ç—å —á–µ—Ç–∫–æ, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ. –£–ø–æ–º—è–Ω–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –µ—Å–ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ.
    `;

    return this.callOllama(this.chatModel, prompt); // –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å
  }

  /**
   * –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞ Python –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤
   * 
   * @param requirement - –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—á–µ—Ç–∞
   * @returns Python –∫–æ–¥
   */
  public async generatePythonCode(requirement: string): Promise<string> {
    const prompt = `
    –ù–∞–ø–∏—à–∏ Python —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è: ${requirement}
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å—Ç—ã–π –∏ —Ö–æ—Ä–æ—à–æ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω
    - –ò—Å–ø–æ–ª—å–∑—É–π type hints
    - –î–æ–±–∞–≤—å docstring
    - –û–±—Ä–∞–±–æ—Ç–∞–π –æ—à–∏–±–∫–∏
    - –í–µ—Ä–Ω–∏ –≥–æ—Ç–æ–≤—ã–π –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∫–æ–¥
    
    –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û —Å –∫–æ–¥–æ–º, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.
    `;

    return this.callOllama(this.analysisModel, prompt);
  }

  /**
   * –û—Å–Ω–æ–≤–Ω–æ–π –≤—ã–∑–æ–≤ –∫ Ollama API (–±–µ–∑ streaming)
   * 
   * @param model - –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   * @param prompt - –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
   * @returns –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
   */
  private async callOllama(model: string, prompt: string): Promise<string> {
    if (!this.isHealthy) {
      // Fallback –Ω–∞ Gemini –µ—Å–ª–∏ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
      console.warn("Ollama server is not healthy, would fallback to Gemini");
      throw new Error(
        "Ollama server is not available. Make sure Ollama is running on http://localhost:11434"
      );
    }

    try {
      const response = await fetch(`${this.ollamaUrl}/generate`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: model,
          prompt: prompt,
          stream: false, // –ë–µ–∑ streaming –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
          temperature: 0.7,
          top_p: 0.95,
          // context: [], // –î–ª—è multi-turn –¥–∏–∞–ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.statusText}`);
      }

      const data = (await response.json()) as OllamaResponse;

      // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
      console.log(`[Ollama ${model}]`, {
        promptTokens: data.prompt_eval_count,
        responseTokens: data.eval_count,
        totalTime: `${(data.total_duration / 1e9).toFixed(2)}s`,
        loadTime: `${(data.load_duration / 1e9).toFixed(2)}s`,
        evalTime: `${(data.eval_duration / 1e9).toFixed(2)}s`,
      });

      return data.response.trim();
    } catch (error) {
      console.error("Error calling Ollama:", error);
      throw error;
    }
  }

  /**
   * Streaming –≤—ã–∑–æ–≤ –∫ Ollama (–¥–ª—è real-time –æ—Ç–≤–µ—Ç–æ–≤)
   * 
   * @param model - –ú–æ–¥–µ–ª—å
   * @param prompt - –ü—Ä–æ–º–ø—Ç
   * @param onChunk - Callback –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
   */
  public async callOllamaStreaming(
    model: string,
    prompt: string,
    onChunk: (chunk: string) => void
  ): Promise<void> {
    if (!this.isHealthy) {
      throw new Error("Ollama server is not available");
    }

    try {
      const response = await fetch(`${this.ollamaUrl}/generate`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: model,
          prompt: prompt,
          stream: true, // ‚Üê –ö–ª—é—á–µ–≤–∞—è —Ä–∞–∑–Ω–∏—Ü–∞!
          temperature: 0.7,
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.statusText}`);
      }

      // –ß–∏—Ç–∞–µ–º streaming response
      const reader = response.body?.getReader();
      if (!reader) throw new Error("No response body");

      const decoder = new TextDecoder();
      let done = false;

      while (!done) {
        const { value, done: streamDone } = await reader.read();
        done = streamDone;

        if (value) {
          const text = decoder.decode(value);
          // –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ JSON –æ–±—ä–µ–∫—Ç
          const lines = text.split("\n").filter((line) => line.trim());

          for (const line of lines) {
            try {
              const json = JSON.parse(line);
              if (json.response) {
                onChunk(json.response);
              }
            } catch (e) {
              // Ignore parse errors
            }
          }
        }
      }
    } catch (error) {
      console.error("Error in streaming call:", error);
      throw error;
    }
  }

  /**
   * Chat API (–¥–ª—è –º–Ω–æ–≥–æ–æ–±–æ—Ä–æ—Ç–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤)
   * 
   * @param model - –ú–æ–¥–µ–ª—å
   * @param messages - –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
   * @returns –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
   */
  public async chat(
    model: string,
    messages: Array<{ role: "user" | "assistant"; content: string }>
  ): Promise<string> {
    if (!this.isHealthy) {
      throw new Error("Ollama server is not available");
    }

    try {
      const response = await fetch(`${this.ollamaUrl}/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: model,
          messages: messages,
          stream: false,
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.statusText}`);
      }

      const data = (await response.json()) as OllamaChatResponse;
      return data.message.content.trim();
    } catch (error) {
      console.error("Error in chat:", error);
      throw error;
    }
  }

  /**
   * –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
   */
  private buildAnalysisPrompt(cabinet: CabinetConfig): string {
    return `
    –¢—ã - –∏–Ω–∂–µ–Ω–µ—Ä-–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫ –º–µ–±–µ–ª–∏ —Å –æ–ø—ã—Ç–æ–º –≤ –¥–µ—Ä–µ–≤–æ–æ–±—Ä–∞–±–æ—Ç–∫–µ.
    
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç—É –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —à–∫–∞—Ñ–∞:
    
    –ü–ê–†–ê–ú–ï–¢–†–´:
    - –†–∞–∑–º–µ—Ä—ã: ${cabinet.width}mm (–®) x ${cabinet.height}mm (–í) x ${cabinet.depth}mm (–ì)
    - –û—Å–Ω–æ–≤–Ω–æ–π –º–∞—Ç–µ—Ä–∏–∞–ª: ${cabinet.material || "–î–°–ü"}
    - –¢–æ–ª—â–∏–Ω–∞ —Å—Ç–µ–Ω–æ–∫: ${cabinet.wallThickness || "18"}mm
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–æ–∫: ${cabinet.shelves ? cabinet.shelves.length : 0}
    - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: ${cabinet.shelves ? JSON.stringify(cabinet.shelves) : "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"}
    
    –¢–†–ï–ë–£–ï–¢–°–Ø:
    1. –û—Ü–µ–Ω–∏—Ç—å –ø—Ä–æ—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º
    3. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω—ã
    4. –û—Ü–µ–Ω–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    5. –£–∫–∞–∑–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ
    
    –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
    –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å:
    - –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: [–æ–ø–∏—Å–∞–Ω–∏–µ]
    - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: [—Å–ø–∏—Å–æ–∫]
    - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: [—Å–ø–∏—Å–æ–∫ —É–ª—É—á—à–µ–Ω–∏–π]
    - –†–∏—Å–∫–∏: [–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã]
    - –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞: [1-10]
    `;
  }

  /**
   * –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É Ollama –∏ Gemini (–¥–ª—è fallback)
   */
  public async isOllamaAvailable(): Promise<boolean> {
    return this.checkHealth();
  }

  /**
   * –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
   */
  public async getModels(): Promise<{ name: string; size: string }[]> {
    try {
      const response = await fetch(`${this.ollamaUrl}/tags`);
      const data = await response.json();
      return data.models || [];
    } catch (error) {
      console.error("Error fetching models:", error);
      return [];
    }
  }
}

// –°–æ–∑–¥–∞—ë–º singleton —ç–∫–∑–µ–º–ø–ª—è—Ä
export const ollamaService = new OllamaService();
```

---

## –≠—Ç–∞–ø 4: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ geminiService.ts

**–î–æ–±–∞–≤—å—Ç–µ fallback –Ω–∞ Ollama –≤ `/services/geminiService.ts`:**

```typescript
// –í –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ –¥–æ–±–∞–≤–∏—Ç—å:
import { ollamaService } from "./ollamaService";

// –í –∫–ª–∞—Å—Å–µ GeminiCabinetService –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥:

/**
 * –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏–Ω–∞—á–µ fallback –Ω–∞ Gemini
 */
private async callWithFallback<T>(
  ollamaCall: () => Promise<T>,
  geminiFallback: () => Promise<T>
): Promise<T> {
  try {
    if (await ollamaService.isOllamaAvailable()) {
      console.log("[AI] Using Ollama (local LLM)");
      return await ollamaCall();
    }
  } catch (error) {
    console.warn("[AI] Ollama failed, falling back to Gemini:", error);
  }

  console.log("[AI] Using Gemini API");
  return await geminiFallback();
}

// –û–±–Ω–æ–≤–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã:

async createDesignFromPrompt(
  description: string,
  context?: any
): Promise<CabinetConfig> {
  return this.callWithFallback(
    async () => {
      const response = await ollamaService.analyzeConstruction({
        width: 400,
        height: 2000,
        depth: 400,
      });
      // –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç –≤ CabinetConfig...
      return {} as CabinetConfig;
    },
    async () => {
      // –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Gemini –∫–æ–¥
      const model = this.client.getGenerativeModel({
        model: "gemini-2.0-flash",
      });
      const response = await model.generateContent(description);
      // ... –æ–±—Ä–∞–±–æ—Ç–∫–∞ ...
      return {} as CabinetConfig;
    }
  );
}

async conductTechnicalAudit(cabinet: CabinetConfig): Promise<string> {
  return this.callWithFallback(
    async () => ollamaService.conductTechnicalAudit(cabinet),
    async () => {
      // –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Gemini –∫–æ–¥...
      return "Audit report...";
    }
  );
}

async askFurnitureExpert(question: string): Promise<string> {
  return this.callWithFallback(
    async () => ollamaService.askFurnitureExpert(question),
    async () => {
      // –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Gemini –∫–æ–¥...
      return "Expert answer...";
    }
  );
}
```

---

## –≠—Ç–∞–ø 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ AIAssistant.tsx

**–û–±–Ω–æ–≤–∏—Ç–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç `/components/AIAssistant.tsx`:**

```typescript
// –î–æ–±–∞–≤–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∫–∞–∫–æ–π AI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

import { ollamaService } from "../services/ollamaService";

export const AIAssistant: React.FC = () => {
  const [aiSource, setAiSource] = useState<"ollama" | "gemini">("gemini");

  useEffect(() => {
    // –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
    const checkAI = async () => {
      const isOllamaAvailable = await ollamaService.isOllamaAvailable();
      setAiSource(isOllamaAvailable ? "ollama" : "gemini");
    };
    checkAI();
  }, []);

  return (
    <div className="ai-assistant">
      {/* –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ AI */}
      <div className="ai-source-badge">
        {aiSource === "ollama" ? (
          <>
            <span className="dot ollama-green"></span>
            Local Ollama (Qwen/Mistral)
          </>
        ) : (
          <>
            <span className="dot gemini-blue"></span>
            Cloud Gemini API
          </>
        )}
      </div>

      {/* –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ */}
      {/* ... */}
    </div>
  );
};

// CSS –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
const styles = `
.ai-source-badge {
  display: inline-flex;
  align-items: center;
  gap: 8px;
  padding: 4px 12px;
  border-radius: 16px;
  font-size: 12px;
  font-weight: 500;
}

.ai-source-badge .dot {
  width: 8px;
  height: 8px;
  border-radius: 50%;
  animation: pulse 2s infinite;
}

.ai-source-badge .ollama-green {
  background: #10b981;
  animation: pulse-green 2s infinite;
}

.ai-source-badge .gemini-blue {
  background: #3b82f6;
  animation: pulse-blue 2s infinite;
}

@keyframes pulse-green {
  0%, 100% { opacity: 1; }
  50% { opacity: 0.5; }
}
`;
```

---

## –≠—Ç–∞–ø 6: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### 6.1 –ë–∞–∑–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ

```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞
curl http://localhost:11434/api/tags

# –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å JSON —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏:
# {"models":[{"name":"qwen2.5:32b-instruct-q5_0",...},{"name":"mistral:7b-instruct-q5_0",...}]}

# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ Qwen:
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:32b-instruct-q5_0",
    "prompt": "–®–∫–∞—Ñ—á–∏–∫ 400x600mm –∏–∑ –î–°–ü 18mm - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º?",
    "stream": false
  }'

# –î–æ–ª–∂–µ–Ω –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç —Å –ø–æ–ª–µ–º "response"

# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ Mistral:
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral:7b-instruct-q5_0",
    "prompt": "–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –∫—É—Ö–Ω–∏?",
    "stream": false
  }'
```

### 6.2 –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ

```typescript
// –í –±—Ä–∞—É–∑–µ—Ä–Ω–æ–π –∫–æ–Ω—Å–æ–ª–∏ (DevTools):

// 1. –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä–≤–∏—Å
import { ollamaService } from "./services/ollamaService";

// 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–¥–æ—Ä–æ–≤—å–µ
await ollamaService.isOllamaAvailable(); // true/false

// 3. –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
const result = await ollamaService.analyzeConstruction({
  width: 400,
  height: 600,
  depth: 350,
  material: "–î–°–ü",
  wallThickness: 18,
});

console.log(result);
// –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
```

### 6.3 Unit —Ç–µ—Å—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

```typescript
// /tests/ollamaService.test.ts

import { describe, it, expect, beforeAll } from "vitest";
import { ollamaService } from "../services/ollamaService";

describe("OllamaService", () => {
  beforeAll(async () => {
    // –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞ –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–∞–º–∏
    const isHealthy = await ollamaService.isOllamaAvailable();
    if (!isHealthy) {
      console.warn("Ollama is not available, tests will be skipped");
    }
  });

  it("should check health of Ollama server", async () => {
    const isHealthy = await ollamaService.isOllamaAvailable();
    expect(typeof isHealthy).toBe("boolean");
  });

  it("should analyze cabinet construction", async () => {
    const result = await ollamaService.analyzeConstruction({
      width: 400,
      height: 600,
      depth: 350,
    });

    expect(result).toBeDefined();
    expect(result.length).toBeGreaterThan(0);
    expect(result).toContain("400"); // –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–∞–∑–º–µ—Ä
  });

  it("should conduct technical audit", async () => {
    const result = await ollamaService.conductTechnicalAudit({
      width: 400,
      height: 600,
      depth: 350,
      material: "–î–°–ü",
    });

    expect(result).toBeDefined();
    expect(result.includes("‚úÖ") || result.includes("‚ö†Ô∏è")).toBe(true);
  });

  it("should get expert recommendations", async () => {
    const result = await ollamaService.askFurnitureExpert(
      "–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –∫—É—Ö–Ω–∏?"
    );

    expect(result).toBeDefined();
    expect(result.length).toBeGreaterThan(10);
  });
});
```

**–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤:**

```bash
npm run test
```

---

## Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞ 1: "Connection refused at localhost:11434"

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# 1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞
ollama serve

# 2. –ï—Å–ª–∏ –Ω–∞ Linux, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ systemd —Å–µ—Ä–≤–∏—Å
sudo systemctl status ollama

# 3. –ï—Å–ª–∏ –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞, –∑–∞–ø—É—Å—Ç–∏—Ç–µ
sudo systemctl start ollama

# 4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ —Å–ª—É—à–∞–µ—Ç –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—Ç—É
netstat -tlnp | grep 11434
# –î–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å: tcp 0 0 127.0.0.1:11434 LISTEN
```

### –ü—Ä–æ–±–ª–µ–º–∞ 2: "Model not found: qwen2.5:32b-instruct-q5_0"

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –µ—ë:
ollama pull qwen2.5:32b-instruct-q5_0

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
ollama list
# –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ —Å–ø–∏—Å–∫–µ
```

### –ü—Ä–æ–±–ª–µ–º–∞ 3: –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (5+ —Å–µ–∫—É–Ω–¥)

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# 1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU (–µ—Å–ª–∏ –µ—Å—Ç—å)
nvidia-smi
# –î–æ–ª–∂–Ω—ã –≤–∏–¥–µ—Ç—å –ø—Ä–æ—Ü–µ—Å—Å ollama —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º VRAM

# 2. –ï—Å–ª–∏ –Ω–µ—Ç GPU, –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU
# –≠—Ç–æ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ. –í–∞—Ä–∏–∞–Ω—Ç—ã:
# - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å (Mistral 7B –≤–º–µ—Å—Ç–æ 32B)
# - –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å quantization: Q4 –≤–º–µ—Å—Ç–æ Q5
ollama pull qwen2.5:7b-instruct-q4_0  # –º–µ–Ω—å—à–µ –∏ –±—ã—Å—Ç—Ä–µ–µ

# 3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–∏—Å—Ç–µ–º—É
top
# –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU/RAM
```

### –ü—Ä–æ–±–ª–µ–º–∞ 4: –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ –ø–∞–º—è—Ç–∏

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# Ollama –≤—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è 5 –º–∏–Ω—É—Ç
# –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–æ –∑–∞–º–µ–¥–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å

# –ß—Ç–æ–±—ã –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–≤–∞–ª–∞—Å—å –≤ –ø–∞–º—è—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä:
# OLLAMA_KEEP_ALIVE=24h

export OLLAMA_KEEP_ALIVE=24h
ollama serve

# –ò–ª–∏ –≤ systemd service (/etc/systemd/system/ollama.service):
Environment="OLLAMA_KEEP_ALIVE=24h"
```

### –ü—Ä–æ–±–ª–µ–º–∞ 5: "Too many requests" –∏–ª–∏ rate limiting

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# Ollama –Ω–µ –∏–º–µ–µ—Ç rate limiting –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
# –ï—Å–ª–∏ –ø–æ–ª—É—á–∞–µ—Ç–µ —ç—Ç—É –æ—à–∏–±–∫—É, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å:
# 1. –û—á–µ–Ω—å –º–Ω–æ–≥–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
# 2. Ollama –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–∞

# –†–µ—à–µ–Ω–∏–µ:
# - –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏
# - –î–æ–±–∞–≤–∏—Ç—å queue –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
# - –ó–∞–ø—É—Å—Ç–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ Ollama –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä—Ç–∞—Ö
```

### –ü—Ä–æ–±–ª–µ–º–∞ 6: –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –û–±–µ –º–æ–¥–µ–ª–∏ (32GB + 14GB) —Ç—Ä–µ–±—É—é—Ç ~25-30GB VRAM

# –ï—Å–ª–∏ –ø–∞–º—è—Ç–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –≤–∞—Ä–∏–∞–Ω—Ç—ã:
# 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–∏–π quantization (Q4 –≤–º–µ—Å—Ç–æ Q5)
ollama pull qwen2.5:32b-instruct-q4_0  # 9.5GB –≤–º–µ—Å—Ç–æ 12GB

# 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
ollama pull qwen2.5:7b-instruct-q5_0   # 4GB –≤–º–µ—Å—Ç–æ 12GB

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—à–∏–Ω–∞—Ö
# - Qwen –Ω–∞ –º–∞—à–∏–Ω–µ 1
# - Mistral –Ω–∞ –º–∞—à–∏–Ω–µ 2

# 4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
nvidia-smi
```

---

## üöÄ DEPLOYMENT –ù–ê PRODUCTION

### Docker Compose (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

**–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `docker-compose.yml` –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞:**

```yaml
version: "3.9"
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-service
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_GPU=1
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ollama-models:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  bazis-web:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bazis-web
    restart: unless-stopped
    ports:
      - "3002:5173"
    environment:
      - VITE_OLLAMA_URL=http://ollama:11434
      - VITE_GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./src:/app/src

volumes:
  ollama-models:
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**

```bash
# 1. –°–æ–∑–¥–∞—Ç—å .env —Ñ–∞–π–ª
echo "GEMINI_API_KEY=your-key-here" > .env

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç—å
docker-compose up -d

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
docker-compose logs -f ollama
docker-compose logs -f bazis-web

# 4. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
docker-compose down
```

### Systemd Unit (–¥–ª—è Linux bare metal)

**–°–æ–∑–¥–∞–π—Ç–µ `/etc/systemd/system/ollama.service`:**

```ini
[Unit]
Description=Ollama Service
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
User=ollama
Group=ollama
ExecStart=/usr/bin/ollama serve
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
WorkingDirectory=/home/ollama

Environment="OLLAMA_NUM_GPU=1"
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_KEEP_ALIVE=24h"
Environment="OLLAMA_MODELS=/data/ollama/models"

[Install]
WantedBy=multi-user.target
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**

```bash
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama

# –õ–æ–≥–∏
sudo journalctl -u ollama -f
```

---

## ‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø CHECKLIST

- [ ] Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ –∑–∞–ø—É—â–µ–Ω–∞
- [ ] –ú–æ–¥–µ–ª–∏ qwen –∏ mistral –∑–∞–≥—Ä—É–∂–µ–Ω—ã
- [ ] ollamaService.ts —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ `/services/`
- [ ] geminiService.ts –æ–±–Ω–æ–≤–ª–µ–Ω —Å fallback
- [ ] AIAssistant.tsx –æ–±–Ω–æ–≤–ª–µ–Ω —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º
- [ ] –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–π–¥–µ–Ω–æ
- [ ] Unit —Ç–µ—Å—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- [ ] Docker Compose –∏–ª–∏ systemd –≥–æ—Ç–æ–≤
- [ ] Production deployment —Å–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω
- [ ] Monitoring –¥–æ–±–∞–≤–ª–µ–Ω (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥–µ –≤—ã–¥–∞–Ω–∞
- [ ] ‚úÖ READY FOR DEPLOYMENT!

---

## üìû –ü–û–î–î–ï–†–ñ–ö–ê

–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã:

1. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏:**
   ```bash
   # Ollama –ª–æ–≥–∏
   sudo journalctl -u ollama -f
   
   # Docker –ª–æ–≥–∏
   docker logs ollama-service
   ```

2. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–¥–æ—Ä–æ–≤—å–µ:**
   ```bash
   curl http://localhost:11434/api/tags
   ```

3. **–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ—Å—å:**
   ```bash
   # –ü—Ä–æ—Å—Ç–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞
   sudo systemctl restart ollama
   
   # Docker –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞
   docker-compose restart ollama
   ```

4. **–û—á–∏—Å—Ç–∏—Ç–µ –∫—ç—à (–µ—Å–ª–∏ –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã):**
   ```bash
   # –£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª–∏
   ollama rm qwen2.5:32b-instruct-q5_0
   ollama rm mistral:7b-instruct-q5_0
   
   # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å
   ollama pull qwen2.5:32b-instruct-q5_0
   ollama pull mistral:7b-instruct-q5_0
   ```

---

**üìã –î–û–ö–£–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù**

*–í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã, –∫–æ–¥—ã –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≥–æ—Ç–æ–≤—ã –∫ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.*

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–û–¢–û–í–û –ö PRODUCTION DEPLOYMENT
