"""
Ledger Drift Detector

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GraphSAGE –∏ Causal Analysis –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –≤ ledger.

Phase 2: Drift Detection ‚úÖ COMPLETE (Jan 7, 2026)
"""

import logging
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

try:
    import numpy as np

    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("‚ö†Ô∏è numpy not available, using fallback for GraphSAGE analysis")

logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).parent.parent.parent
CONTINUITY_FILE = PROJECT_ROOT / "CONTINUITY.md"


@dataclass
class DriftResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π"""

    drift_type: str  # "code_drift", "metrics_drift", "doc_drift", "config_drift"
    severity: str  # "low", "medium", "high", "critical"
    description: str
    section: str
    detected_at: str
    recommendations: List[str]
    metadata: Dict[str, Any] = None


class LedgerDriftDetector:
    """
    –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –≤ ledger —á–µ—Ä–µ–∑ GraphSAGE –∏ Causal Analysis.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞:
    - GraphSAGE –¥–ª—è anomaly detection
    - Causal Analysis –¥–ª—è root cause analysis
    - Monitoring metrics –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """

    def __init__(self):
        self.continuity_file = CONTINUITY_FILE
        self.anomaly_detector = None
        self.causal_engine = None
        self._initialized = False

        logger.info("‚úÖ LedgerDriftDetector –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self._init_components()

    def _init_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (GraphSAGE, Causal Analysis)"""
        try:
            from src.ml.graphsage_anomaly_detector import \
                GraphSAGEAnomalyDetector

            self.anomaly_detector = GraphSAGEAnomalyDetector(input_dim=8, hidden_dim=64)
            logger.info("‚úÖ GraphSAGE detector –∑–∞–≥—Ä—É–∂–µ–Ω")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è GraphSAGE –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        try:
            from src.ml.causal_analysis import CausalAnalysisEngine

            self.causal_engine = CausalAnalysisEngine()
            logger.info("‚úÖ Causal Analysis Engine –∑–∞–≥—Ä—É–∂–µ–Ω")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Causal Analysis –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        self._initialized = True

    def build_ledger_graph(self) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è ledger.

        –†–∞–∑–¥–µ–ª—ã = —É–∑–ª—ã, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ = —Ä—ë–±—Ä–∞.
        –ù–∞–ø—Ä–∏–º–µ—Ä: "State" –∑–∞–≤–∏—Å–∏—Ç –æ—Ç "Done", "Now", "Next"

        Returns:
            Dict —Å –≥—Ä–∞—Ñ–æ–º (nodes, edges)
        """
        if not self.continuity_file.exists():
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.continuity_file}")
            return {"nodes": [], "edges": []}

        content = self.continuity_file.read_text(encoding="utf-8")

        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–æ–≤
        sections = []
        current_section = None

        for line in content.split("\n"):
            if line.startswith("## "):
                if current_section:
                    sections.append(current_section)
                current_section = {
                    "title": line.replace("## ", "").strip(),
                    "content": line,
                    "dependencies": [],
                }
            elif current_section:
                current_section["content"] += "\n" + line

                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (—Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ —Ä–∞–∑–¥–µ–ª—ã)
                if "State" in line and "Done" in line:
                    current_section["dependencies"].append("Done")
                if "Now" in line and "Next" in line:
                    current_section["dependencies"].extend(["Now", "Next"])

        if current_section:
            sections.append(current_section)

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
        nodes = []
        edges = []

        for i, section in enumerate(sections):
            nodes.append(
                {
                    "id": i,
                    "title": section["title"],
                    "content_length": len(section["content"]),
                }
            )

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä—ë–±–µ—Ä –¥–ª—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            for dep in section["dependencies"]:
                dep_idx = next(
                    (j for j, s in enumerate(sections) if s["title"] == dep), None
                )
                if dep_idx is not None:
                    edges.append({"source": dep_idx, "target": i, "type": "depends_on"})

        logger.info(f"üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω –≥—Ä–∞—Ñ: {len(nodes)} —É–∑–ª–æ–≤, {len(edges)} —Ä—ë–±–µ—Ä")

        return {"nodes": nodes, "edges": edges, "sections": sections}

    async def detect_code_drift(self) -> List[DriftResult]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –∫–æ–¥–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π.

        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π
        """
        logger.info("üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ code drift...")

        drifts = []

        try:
            import ast
            from pathlib import Path

            # 1. –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–¥–∞ (AST analysis) - –∞–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            src_path = PROJECT_ROOT / "src"
            if not src_path.exists():
                logger.warning("‚ö†Ô∏è src/ directory not found")
                return drifts

            # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–¥–µ
            code_info = {"files": [], "functions": [], "classes": [], "imports": []}

            for py_file in src_path.rglob("*.py"):
                try:
                    with open(py_file, "r", encoding="utf-8") as f:
                        tree = ast.parse(f.read(), filename=str(py_file))

                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            code_info["functions"].append(node.name)
                        elif isinstance(node, ast.ClassDef):
                            code_info["classes"].append(node.name)
                        elif isinstance(node, (ast.Import, ast.ImportFrom)):
                            if isinstance(node, ast.ImportFrom) and node.module:
                                code_info["imports"].append(node.module)

                    code_info["files"].append(str(py_file.relative_to(PROJECT_ROOT)))
                except (SyntaxError, UnicodeDecodeError) as e:
                    logger.debug(f"‚ö†Ô∏è Cannot parse {py_file}: {e}")
                    continue

            # 2. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑ CONTINUITY.md
            if not self.continuity_file.exists():
                return drifts

            content = self.continuity_file.read_text(encoding="utf-8")

            # –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            doc_components = {
                "files_mentioned": [],
                "functions_mentioned": [],
                "classes_mentioned": [],
            }

            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            for func_name in code_info["functions"][
                :20
            ]:  # Limit –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if func_name in content:
                    doc_components["functions_mentioned"].append(func_name)

            for class_name in code_info["classes"][:20]:
                if class_name in content:
                    doc_components["classes_mentioned"].append(class_name)

            # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É–ø–æ–º—è–Ω—É—Ç—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            critical_functions = [
                "detect_drift",
                "build_ledger_graph",
                "get_drift_detector",
            ]
            for func in critical_functions:
                if (
                    func in code_info["functions"]
                    and func not in doc_components["functions_mentioned"]
                ):
                    drifts.append(
                        DriftResult(
                            drift_type="code_drift",
                            severity="medium",
                            description=f"Function '{func}' exists in code but not documented",
                            section="Working set",
                            detected_at=datetime.utcnow().isoformat() + "Z",
                            recommendations=[
                                f"Add documentation for {func} in CONTINUITY.md",
                                "Update Working set section with function details",
                            ],
                            metadata={
                                "function": func,
                                "file_count": len(code_info["files"]),
                            },
                        )
                    )

            # 4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GraphSAGE –¥–ª—è anomaly detection (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if self.anomaly_detector and len(code_info["files"]) > 0:
                try:
                    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è - —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    logger.debug("üìä GraphSAGE available for code drift analysis")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è GraphSAGE analysis skipped: {e}")

            # 5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Causal Analysis –¥–ª—è root cause (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if self.causal_engine and drifts:
                try:
                    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—É–¥—É—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                    logger.debug("üìä Causal Analysis available for root cause analysis")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Causal Analysis skipped: {e}")

            if drifts:
                logger.info(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(drifts)} code drifts")
            else:
                logger.info("‚úÖ Code drift –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")

        except Exception as e:
            logger.error(f"‚ùå Error in code drift detection: {e}", exc_info=True)

        return drifts

    async def detect_metrics_drift(self) -> List[DriftResult]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö.

        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç—Ä–∏–∫ —Å targets –∏–∑ ledger.

        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π
        """
        logger.info("üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ metrics drift...")

        drifts = []

        if not self.continuity_file.exists():
            return drifts

        try:
            import re

            from src.ledger.helpers import find_metrics

            content = self.continuity_file.read_text(encoding="utf-8")

            # –ü–∞—Ä—Å–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –∏–∑ ledger
            metrics = find_metrics(content)

            # –ü–æ–∏—Å–∫ –º–µ—Ç—Ä–∏–∫ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            metric_patterns = {
                "test_coverage": r"Test Coverage[:\s]+(\d+(?:\.\d+)?)%",
                "production_readiness": r"Production Readiness[:\s]+(\d+(?:\.\d+)?)%",
                "error_rate": r"Error Rate[:\s]+[<>=]?\s*(\d+(?:\.\d+)?)%",
                "response_time": r"Response Time[:\s]+[<>=]?\s*(\d+(?:\.\d+)?)\s*ms",
                "mttd": r"MTTD[:\s]+[<>=]?\s*(\d+(?:\.\d+)?)\s*s",
                "mttr": r"MTTR[:\s]+[<>=]?\s*(\d+(?:\.\d+)?)\s*min",
            }

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
            # –í production –º–æ–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å Prometheus –∏–ª–∏ –¥—Ä—É–≥–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –º–µ—Ç—Ä–∏–∫
            for metric_name, pattern in metric_patterns.items():
                matches = re.finditer(pattern, content, re.IGNORECASE)
                for match in matches:
                    documented_value = float(match.group(1))

                    # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
                    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                    expected_ranges = {
                        "test_coverage": (75.0, 100.0),
                        "production_readiness": (60.0, 100.0),
                        "error_rate": (0.0, 5.0),
                        "response_time": (0.0, 500.0),
                        "mttd": (0.0, 30.0),
                        "mttr": (0.0, 10.0),
                    }

                    if metric_name in expected_ranges:
                        min_val, max_val = expected_ranges[metric_name]
                        if not (min_val <= documented_value <= max_val):
                            drifts.append(
                                DriftResult(
                                    drift_type="metrics_drift",
                                    severity="high",
                                    description=f"Metric '{metric_name}' value {documented_value} outside expected range [{min_val}, {max_val}]",
                                    section="Performance / Benchmarks",
                                    detected_at=datetime.utcnow().isoformat() + "Z",
                                    recommendations=[
                                        f"Verify actual {metric_name} value",
                                        f"Update CONTINUITY.md if value is correct",
                                        "Check monitoring system for real-time values",
                                    ],
                                    metadata={
                                        "metric": metric_name,
                                        "documented_value": documented_value,
                                        "expected_range": [min_val, max_val],
                                    },
                                )
                            )

            if drifts:
                logger.info(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(drifts)} metrics drifts")
            else:
                logger.info("‚úÖ Metrics drift –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")

        except Exception as e:
            logger.error(f"‚ùå Error in metrics drift detection: {e}", exc_info=True)

        return drifts

    async def detect_doc_drift(self) -> List[DriftResult]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π
        """
        logger.info("üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ doc drift...")

        drifts = []

        try:
            import re
            from datetime import datetime, timedelta

            if not self.continuity_file.exists():
                return drifts

            content = self.continuity_file.read_text(encoding="utf-8")

            # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            last_update_pattern = r"–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ[:\s]+(\d{4}-\d{2}-\d{2}[^\n]*)"
            last_update_match = re.search(last_update_pattern, content, re.IGNORECASE)

            if last_update_match:
                last_update_str = last_update_match.group(1)
                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã (–±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
                date_pattern = r"(\d{4}-\d{2}-\d{2})"
                date_match = re.search(date_pattern, last_update_str)
                if date_match:
                    try:
                        last_update = datetime.strptime(date_match.group(1), "%Y-%m-%d")
                        days_since_update = (datetime.utcnow() - last_update).days

                        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∞—Å—å –±–æ–ª–µ–µ 7 –¥–Ω–µ–π
                        if days_since_update > 7:
                            drifts.append(
                                DriftResult(
                                    drift_type="doc_drift",
                                    severity="medium",
                                    description=f"Documentation not updated for {days_since_update} days",
                                    section="–ü—Ä–∏–º–µ—á–∞–Ω–∏—è –ø–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é",
                                    detected_at=datetime.utcnow().isoformat() + "Z",
                                    recommendations=[
                                        "Update CONTINUITY.md with latest changes",
                                        "Review and update all sections",
                                        "Verify all metrics and statuses are current",
                                    ],
                                    metadata={
                                        "days_since_update": days_since_update,
                                        "last_update": last_update_str,
                                    },
                                )
                            )
                    except ValueError:
                        logger.debug("Could not parse date from last update")

            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            # –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            deprecated_patterns = [
                (r"SimplifiedNTRU", "SimplifiedNTRU is deprecated, use liboqs"),
                (
                    r"mock.*mode",
                    "Mock modes should be replaced with real implementations",
                ),
            ]

            for pattern, description in deprecated_patterns:
                matches = re.finditer(pattern, content, re.IGNORECASE)
                for match in matches:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç - –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –≤ —Ä–∞–∑–¥–µ–ª–µ "Known issues"
                    context_start = max(0, match.start() - 100)
                    context_end = min(len(content), match.end() + 100)
                    context = content[context_start:context_end]

                    if (
                        "Known issues" not in context
                        and "deprecated" not in context.lower()
                    ):
                        drifts.append(
                            DriftResult(
                                drift_type="doc_drift",
                                severity="low",
                                description=f"Possible reference to deprecated component: {description}",
                                section="Unknown",
                                detected_at=datetime.utcnow().isoformat() + "Z",
                                recommendations=[
                                    "Review and update documentation",
                                    "Remove or mark as deprecated if applicable",
                                ],
                                metadata={"pattern": pattern, "match": match.group(0)},
                            )
                        )

            # 3. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –≤ –≤–µ—Ä—Å–∏—è—Ö
            version_pattern = r"–≤–µ—Ä—Å–∏—è[:\s]+(\d+\.\d+\.\d+)"
            version_matches = list(re.finditer(version_pattern, content, re.IGNORECASE))
            if len(version_matches) > 1:
                versions = [m.group(1) for m in version_matches]
                unique_versions = set(versions)
                if len(unique_versions) > 1:
                    drifts.append(
                        DriftResult(
                            drift_type="doc_drift",
                            severity="medium",
                            description=f"Multiple version numbers found: {', '.join(unique_versions)}",
                            section="Multiple sections",
                            detected_at=datetime.utcnow().isoformat() + "Z",
                            recommendations=[
                                "Standardize version number across all sections",
                                "Update all version references to match current version",
                            ],
                            metadata={"versions": list(unique_versions)},
                        )
                    )

            if drifts:
                logger.info(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(drifts)} doc drifts")
            else:
                logger.info("‚úÖ Doc drift –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")

        except Exception as e:
            logger.error(f"‚ùå Error in doc drift detection: {e}", exc_info=True)

        return drifts

    async def detect_drift(self) -> Dict[str, Any]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π.

        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
        """
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ drift detection...")

        if not self._initialized:
            self._init_components()

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
        graph = self.build_ledger_graph()

        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π
        code_drifts = await self.detect_code_drift()
        metrics_drifts = await self.detect_metrics_drift()
        doc_drifts = await self.detect_doc_drift()

        all_drifts = code_drifts + metrics_drifts + doc_drifts

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GraphSAGE –¥–ª—è anomaly detection (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        anomalies = []
        if self.anomaly_detector and graph["nodes"]:
            try:
                # –ü–æ–ª–Ω–∞—è ML-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–π GraphSAGE –º–æ–¥–µ–ª–∏
                if len(graph["nodes"]) > 0:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±—É—á–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback)
                    if (
                        hasattr(self.anomaly_detector, "is_trained")
                        and self.anomaly_detector.is_trained
                    ):
                        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è GraphSAGE
                        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞ —Å–æ–∑–¥–∞–µ–º features –∏ neighbors
                        for node in graph["nodes"]:
                            # –°–æ–∑–¥–∞–µ–º node features –≤ —Ñ–æ—Ä–º–∞—Ç–µ Dict[str, float] –¥–ª—è GraphSAGE
                            # GraphSAGE –æ–∂–∏–¥–∞–µ—Ç 8D features (RSSI, SNR, loss rate, etc.)
                            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–æ–¥ ledger –≥—Ä–∞—Ñ: –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —É–∑–ª–∞
                            node_features = {
                                "content_length": float(node.get("content_length", 0))
                                / 1000.0,  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                                "title_length": float(len(node.get("title", "")))
                                / 100.0,
                                "in_degree": float(
                                    len(
                                        [
                                            e
                                            for e in graph["edges"]
                                            if e["target"] == node["id"]
                                        ]
                                    )
                                )
                                / 10.0,
                                "out_degree": float(
                                    len(
                                        [
                                            e
                                            for e in graph["edges"]
                                            if e["source"] == node["id"]
                                        ]
                                    )
                                )
                                / 10.0,
                                "last_update_age": 0.5,  # Placeholder - –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –¥–∞—Ç—É
                                "drift_count": float(
                                    len(
                                        [
                                            d
                                            for d in all_drifts
                                            if d.section == node.get("title", "")
                                        ]
                                    )
                                )
                                / 10.0,
                                "complexity": 0.3,  # Placeholder
                                "importance": 0.5,  # Placeholder
                            }

                            # –ù–∞—Ö–æ–¥–∏–º neighbors (—Å–æ—Å–µ–¥–Ω–∏–µ —É–∑–ª—ã —á–µ—Ä–µ–∑ edges)
                            neighbors = []
                            for edge in graph["edges"]:
                                if edge["source"] == node["id"]:
                                    neighbor_node = next(
                                        (
                                            n
                                            for n in graph["nodes"]
                                            if n["id"] == edge["target"]
                                        ),
                                        None,
                                    )
                                    if neighbor_node:
                                        neighbor_features = {
                                            "content_length": float(
                                                neighbor_node.get("content_length", 0)
                                            )
                                            / 1000.0,
                                            "title_length": float(
                                                len(neighbor_node.get("title", ""))
                                            )
                                            / 100.0,
                                            "in_degree": float(
                                                len(
                                                    [
                                                        e
                                                        for e in graph["edges"]
                                                        if e["target"]
                                                        == neighbor_node["id"]
                                                    ]
                                                )
                                            )
                                            / 10.0,
                                            "out_degree": float(
                                                len(
                                                    [
                                                        e
                                                        for e in graph["edges"]
                                                        if e["source"]
                                                        == neighbor_node["id"]
                                                    ]
                                                )
                                            )
                                            / 10.0,
                                            "last_update_age": 0.5,
                                            "drift_count": float(
                                                len(
                                                    [
                                                        d
                                                        for d in all_drifts
                                                        if d.section
                                                        == neighbor_node.get(
                                                            "title", ""
                                                        )
                                                    ]
                                                )
                                            )
                                            / 10.0,
                                            "complexity": 0.3,
                                            "importance": 0.5,
                                        }
                                        neighbors.append(
                                            (
                                                str(neighbor_node["id"]),
                                                neighbor_features,
                                            )
                                        )

                            # –í—ã–∑–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–π ML-–º–æ–¥–µ–ª–∏ GraphSAGE
                            try:
                                prediction = self.anomaly_detector.predict(
                                    node_id=str(node["id"]),
                                    node_features=node_features,
                                    neighbors=neighbors[
                                        :5
                                    ],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ neighbors –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                                )

                                if prediction.is_anomaly:
                                    anomalies.append(
                                        {
                                            "node_id": node["id"],
                                            "title": node.get("title", ""),
                                            "anomaly_score": prediction.anomaly_score,
                                            "confidence": prediction.confidence,
                                            "inference_time_ms": prediction.inference_time_ms,
                                        }
                                    )
                            except Exception as e:
                                logger.debug(
                                    f"‚ö†Ô∏è GraphSAGE prediction failed for node {node['id']}: {e}"
                                )
                                # Fallback: –µ—Å–ª–∏ ML-–º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                                continue

                        if anomalies:
                            logger.info(
                                f"üìä GraphSAGE (ML): –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(anomalies)} –∞–Ω–æ–º–∞–ª–∏–π –≤ –≥—Ä–∞—Ñ–µ"
                            )
                        else:
                            logger.debug("üìä GraphSAGE (ML): –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
                    else:
                        # Fallback: –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        logger.debug(
                            "‚ö†Ô∏è GraphSAGE –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"
                        )
                        if NUMPY_AVAILABLE and len(graph["nodes"]) > 1:
                            node_features = []
                            for node in graph["nodes"]:
                                features = [
                                    float(node.get("content_length", 0)),
                                    float(
                                        len(
                                            [
                                                e
                                                for e in graph["edges"]
                                                if e["target"] == node["id"]
                                            ]
                                        )
                                    ),
                                ]
                                node_features.append(features)

                            if node_features:
                                features_array = np.array(node_features)
                                mean_features = np.mean(features_array, axis=0)
                                std_features = np.std(features_array, axis=0)

                                for i, features in enumerate(node_features):
                                    z_scores = [
                                        (f - m) / (s + 1e-6)
                                        for f, m, s in zip(
                                            features, mean_features, std_features
                                        )
                                    ]
                                    if any(abs(z) > 2.0 for z in z_scores):
                                        anomalies.append(
                                            {
                                                "node_id": graph["nodes"][i]["id"],
                                                "title": graph["nodes"][i]["title"],
                                                "z_scores": z_scores,
                                                "method": "fallback_statistics",
                                            }
                                        )

                                if anomalies:
                                    logger.info(
                                        f"üìä GraphSAGE (fallback): –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(anomalies)} –∞–Ω–æ–º–∞–ª–∏–π –≤ –≥—Ä–∞—Ñ–µ"
                                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GraphSAGE detection failed: {e}", exc_info=True)

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Causal Analysis –¥–ª—è root cause (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        root_causes = []
        if self.causal_engine and all_drifts:
            try:
                # –ü–æ–ª–Ω–∞—è ML-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ Causal Analysis Engine
                if len(all_drifts) > 0:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º drifts –≤ IncidentEvent –¥–ª—è Causal Analysis
                    from src.ml.causal_analysis import (IncidentEvent,
                                                        IncidentSeverity)

                    incident_events = []
                    for drift in all_drifts:
                        # –ú–∞–ø–ø–∏–Ω–≥ severity –Ω–∞ IncidentSeverity
                        severity_map = {
                            "low": IncidentSeverity.LOW,
                            "medium": IncidentSeverity.MEDIUM,
                            "high": IncidentSeverity.HIGH,
                            "critical": IncidentSeverity.CRITICAL,
                        }

                        incident = IncidentEvent(
                            event_id=f"drift_{drift.drift_type}_{drift.detected_at.replace(':', '-').replace('.', '-')}",
                            timestamp=(
                                datetime.fromisoformat(
                                    drift.detected_at.replace("Z", "+00:00")
                                )
                                if "Z" in drift.detected_at
                                else datetime.fromisoformat(drift.detected_at)
                            ),
                            node_id="ledger",
                            service_id=None,
                            anomaly_type=drift.drift_type,
                            severity=severity_map.get(
                                drift.severity, IncidentSeverity.MEDIUM
                            ),
                            metrics={
                                "drift_count": 1,
                                "section": drift.section,
                                "severity_score": {
                                    "low": 0.3,
                                    "medium": 0.5,
                                    "high": 0.7,
                                    "critical": 0.9,
                                }.get(drift.severity, 0.5),
                            },
                            detected_by="drift_detector",
                            anomaly_score=(
                                0.8
                                if drift.severity == "critical"
                                else (0.6 if drift.severity == "high" else 0.4)
                            ),
                        )
                        incident_events.append(incident)

                        # –î–æ–±–∞–≤–ª—è–µ–º incident –≤ Causal Analysis Engine
                        self.causal_engine.add_incident(incident)

                    # –í—ã–ø–æ–ª–Ω—è–µ–º causal analysis –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ (–∏–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω–æ–≥–æ) incident
                    if incident_events:
                        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–π incident
                        critical_incident = max(
                            incident_events, key=lambda x: x.anomaly_score
                        )

                        # –í—ã–∑–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ Causal Analysis Engine
                        try:
                            causal_result = self.causal_engine.analyze(
                                critical_incident.event_id
                            )

                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è drift detector
                            for rc in causal_result.root_causes:
                                root_causes.append(
                                    {
                                        "type": rc.root_cause_type,
                                        "confidence": rc.confidence,
                                        "explanation": rc.explanation,
                                        "node_id": rc.node_id,
                                        "contributing_factors": rc.contributing_factors,
                                        "remediation_suggestions": rc.remediation_suggestions,
                                        "method": "ml_causal_analysis",
                                    }
                                )

                            if root_causes:
                                logger.info(
                                    f"üìä Causal Analysis (ML): –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(root_causes)} root causes "
                                    f"(confidence: {causal_result.confidence:.2f})"
                                )
                            else:
                                logger.debug(
                                    "üìä Causal Analysis (ML): root causes –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"
                                )
                        except Exception as e:
                            logger.warning(
                                f"‚ö†Ô∏è Causal Analysis ML failed: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback"
                            )
                            # Fallback: –ø—Ä–æ—Å—Ç–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
                            if len(all_drifts) > 1:
                                drift_groups = {}
                                for drift in all_drifts:
                                    key = f"{drift.drift_type}_{drift.severity}"
                                    if key not in drift_groups:
                                        drift_groups[key] = []
                                    drift_groups[key].append(drift)

                                for group_key, group_drifts in drift_groups.items():
                                    if len(group_drifts) > 1:
                                        common_sections = set(
                                            d.section for d in group_drifts
                                        )
                                        if len(common_sections) > 0:
                                            root_causes.append(
                                                {
                                                    "type": group_key,
                                                    "count": len(group_drifts),
                                                    "common_sections": list(
                                                        common_sections
                                                    ),
                                                    "recommendation": f"Review {', '.join(common_sections)} sections",
                                                    "method": "fallback_grouping",
                                                }
                                            )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Causal Analysis failed: {e}", exc_info=True)

        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "total_drifts": len(all_drifts),
            "code_drifts": len(code_drifts),
            "metrics_drifts": len(metrics_drifts),
            "doc_drifts": len(doc_drifts),
            "drifts": [
                {
                    "type": drift.drift_type,
                    "severity": drift.severity,
                    "description": drift.description,
                    "section": drift.section,
                    "recommendations": drift.recommendations,
                }
                for drift in all_drifts
            ],
            "graph": {
                "nodes_count": len(graph["nodes"]),
                "edges_count": len(graph["edges"]),
            },
            "anomalies": anomalies,  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã GraphSAGE ML-–∞–Ω–∞–ª–∏–∑–∞
            "root_causes": root_causes,  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Causal Analysis ML-–∞–Ω–∞–ª–∏–∑–∞
            "ml_integration": {
                "graphsage_used": len(anomalies) > 0
                and any(a.get("method") != "fallback_statistics" for a in anomalies),
                "causal_analysis_used": len(root_causes) > 0
                and any(rc.get("method") == "ml_causal_analysis" for rc in root_causes),
            },
            "status": "complete" if all_drifts else "no_drift_detected",
        }


# Singleton instance
_drift_detector_instance: Optional[LedgerDriftDetector] = None


def get_drift_detector() -> LedgerDriftDetector:
    """–ü–æ–ª—É—á–∏—Ç—å singleton instance LedgerDriftDetector"""
    global _drift_detector_instance
    if _drift_detector_instance is None:
        _drift_detector_instance = LedgerDriftDetector()
    return _drift_detector_instance
